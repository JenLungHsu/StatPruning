/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name tf_efficientnetv2_s_in21ft1k to current tf_efficientnetv2_s.in21k_ft_in1k.
  model = create_fn(
/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
EfficientNet(
  (conv_stem): Conv2dSame(3, 24, kernel_size=(3, 3), stride=(2, 2), bias=False)
  (bn1): BatchNormAct2d(
    24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
    (drop): Identity()
    (act): SiLU(inplace=True)
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): ConvBnAct(
        (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (drop_path): Identity()
      )
      (1): ConvBnAct(
        (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): EdgeResidual(
        (conv_exp): Conv2dSame(24, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn1): BatchNormAct2d(
          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): Identity()
        (conv_pwl): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNormAct2d(
          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (1): EdgeResidual(
        (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNormAct2d(
          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (2): EdgeResidual(
        (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNormAct2d(
          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (3): EdgeResidual(
        (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNormAct2d(
          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
    )
    (2): Sequential(
      (0): EdgeResidual(
        (conv_exp): Conv2dSame(48, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn1): BatchNormAct2d(
          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): Identity()
        (conv_pwl): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNormAct2d(
          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (1): EdgeResidual(
        (conv_exp): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): Identity()
        (conv_pwl): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNormAct2d(
          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (2): EdgeResidual(
        (conv_exp): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): Identity()
        (conv_pwl): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNormAct2d(
          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (3): EdgeResidual(
        (conv_exp): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): Identity()
        (conv_pwl): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNormAct2d(
          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)
        (bn2): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (bn2): BatchNormAct2d(
          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (bn2): BatchNormAct2d(
          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (bn2): BatchNormAct2d(
          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (bn2): BatchNormAct2d(
          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (bn2): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (bn2): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (bn2): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (4): InvertedResidual(
        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (bn2): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (5): InvertedResidual(
        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (bn2): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (6): InvertedResidual(
        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (bn2): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2dSame(960, 960, kernel_size=(3, 3), stride=(2, 2), groups=960, bias=False)
        (bn2): BatchNormAct2d(
          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (bn2): BatchNormAct2d(
          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (bn2): BatchNormAct2d(
          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (bn2): BatchNormAct2d(
          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (4): InvertedResidual(
        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (bn2): BatchNormAct2d(
          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
      (5): InvertedResidual(
        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNormAct2d(
          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (bn2): BatchNormAct2d(
          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): SiLU(inplace=True)
        )
        (aa): Identity()
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNormAct2d(
          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
          (drop): Identity()
          (act): Identity()
        )
        (drop_path): Identity()
      )
    )
  )
  (conv_head): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNormAct2d(
    1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
    (drop): Identity()
    (act): SiLU(inplace=True)
  )
  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1280, out_features=1000, bias=True)
)
DALI "gpu" variant
EfficientNet(
  (features): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
    (1): Sequential(
      (0): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
      (1): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.005, mode=row)
      )
    )
    (2): Sequential(
      (0): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.01, mode=row)
      )
      (1): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.015000000000000003, mode=row)
      )
      (2): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.02, mode=row)
      )
      (3): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.025, mode=row)
      )
    )
    (3): Sequential(
      (0): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.030000000000000006, mode=row)
      )
      (1): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.035, mode=row)
      )
      (2): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04, mode=row)
      )
      (3): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.045, mode=row)
      )
    )
    (4): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05500000000000001, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.065, mode=row)
      )
      (4): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07, mode=row)
      )
      (5): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.075, mode=row)
      )
    )
    (5): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.085, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.095, mode=row)
      )
      (4): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1, mode=row)
      )
      (5): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10500000000000001, mode=row)
      )
      (6): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11000000000000001, mode=row)
      )
      (7): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11500000000000002, mode=row)
      )
      (8): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)
      )
    )
    (6): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.125, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.135, mode=row)
      )
    )
    (7): Conv2dNormActivation(
      (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
DALI "gpu" variant
pip_train.epoch_size("Reader"):1281167
/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/plugin/base_iterator.py:208: Warning: Please set `reader_name` and don't set last_batch_padded and size manually whenever possible. This may lead, in some situations, to missing some samples or returning duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
current lr 1.00000e-04
Epoch: [0][0/8542]	Time 3.209 (3.209)	Data 0.000 (0.000)	Loss 3.8266 (3.8266)	Prec@1 49.333 (49.333)
Epoch: [0][50/8542]	Time 0.292 (0.350)	Data 0.001 (0.001)	Loss 3.8756 (3.7695)	Prec@1 51.333 (48.837)
Epoch: [0][100/8542]	Time 0.295 (0.322)	Data 0.001 (0.001)	Loss 3.4779 (3.6967)	Prec@1 52.667 (49.670)
Epoch: [0][150/8542]	Time 0.301 (0.314)	Data 0.001 (0.001)	Loss 3.3600 (3.6419)	Prec@1 50.000 (50.274)
Epoch: [0][200/8542]	Time 0.305 (0.311)	Data 0.004 (0.001)	Loss 3.5052 (3.5951)	Prec@1 50.667 (50.922)
Epoch: [0][250/8542]	Time 0.304 (0.310)	Data 0.001 (0.001)	Loss 3.2473 (3.5520)	Prec@1 55.333 (51.413)
Epoch: [0][300/8542]	Time 0.306 (0.309)	Data 0.001 (0.001)	Loss 3.4457 (3.5200)	Prec@1 48.667 (51.617)
Epoch: [0][350/8542]	Time 0.307 (0.309)	Data 0.001 (0.001)	Loss 3.4477 (3.4919)	Prec@1 49.333 (51.968)
Epoch: [0][400/8542]	Time 0.307 (0.309)	Data 0.001 (0.001)	Loss 3.4168 (3.4671)	Prec@1 48.667 (52.170)
Epoch: [0][450/8542]	Time 0.308 (0.309)	Data 0.001 (0.001)	Loss 3.2944 (3.4450)	Prec@1 52.667 (52.378)
Traceback (most recent call last):
  File "trainer_imagenet_efficientnetv2s.py", line 458, in <module>
    main()
  File "trainer_imagenet_efficientnetv2s.py", line 272, in main
    train(train_loader, model, criterion_smooth, optimizer, epoch)
  File "trainer_imagenet_efficientnetv2s.py", line 310, in train
    for i, batch in enumerate(train_loader):
  File "/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/plugin/pytorch/__init__.py", line 180, in __next__
    outputs = self._get_outputs()
  File "/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/plugin/base_iterator.py", line 385, in _get_outputs
    outputs.append(p.share_outputs())
  File "/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/pipeline.py", line 1297, in share_outputs
    return self._pipe.ShareOutputs(types._raw_cuda_stream_ptr(cuda_stream))
RuntimeError: Critical error in pipeline:
Error in MIXED operator `nvidia.dali.ops.decoders.Image`,
which was used in the pipeline definition with the following traceback:

  File "/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/pipeline.py", line 819, in _build_graph
    outputs = define_graph()
  File "/ssd5/Roy/pytorch_resnet_cifar10-master/ImageNet_dali.py", line 24, in define_graph
    images = self.decode(self.jpegs)

encountered:

Error in thread 8: nvJPEG error (5): The user-provided allocator functions, for either memory allocation or for releasing the memory, returned a non-zero code.
Details:
n04418357/n04418357_27940.JPEG
Current pipeline object is no longer valid.
EfficientNet(
  (features): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
    (1): Sequential(
      (0): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
      (1): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.005, mode=row)
      )
    )
    (2): Sequential(
      (0): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.01, mode=row)
      )
      (1): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.015000000000000003, mode=row)
      )
      (2): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.02, mode=row)
      )
      (3): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.025, mode=row)
      )
    )
    (3): Sequential(
      (0): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.030000000000000006, mode=row)
      )
      (1): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.035, mode=row)
      )
      (2): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04, mode=row)
      )
      (3): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.045, mode=row)
      )
    )
    (4): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05500000000000001, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.065, mode=row)
      )
      (4): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07, mode=row)
      )
      (5): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.075, mode=row)
      )
    )
    (5): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.085, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.095, mode=row)
      )
      (4): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1, mode=row)
      )
      (5): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10500000000000001, mode=row)
      )
      (6): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11000000000000001, mode=row)
      )
      (7): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11500000000000002, mode=row)
      )
      (8): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)
      )
    )
    (6): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.125, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.135, mode=row)
      )
    )
    (7): Conv2dNormActivation(
      (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
DALI "gpu" variant
pip_train.epoch_size("Reader"):1281167
/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/plugin/base_iterator.py:208: Warning: Please set `reader_name` and don't set last_batch_padded and size manually whenever possible. This may lead, in some situations, to missing some samples or returning duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
current lr 1.00000e-04
Epoch: [0][0/10010]	Time 2.921 (2.921)	Data 0.001 (0.001)	Loss 3.7405 (3.7405)	Prec@1 50.781 (50.781)
Epoch: [0][50/10010]	Time 0.284 (0.333)	Data 0.001 (0.001)	Loss 3.8057 (3.7813)	Prec@1 46.875 (48.836)
Epoch: [0][100/10010]	Time 0.281 (0.308)	Data 0.001 (0.001)	Loss 3.4076 (3.7012)	Prec@1 55.469 (50.178)
Epoch: [0][150/10010]	Time 0.288 (0.300)	Data 0.001 (0.001)	Loss 3.6948 (3.6540)	Prec@1 48.438 (50.616)
Epoch: [0][200/10010]	Time 0.284 (0.296)	Data 0.001 (0.001)	Loss 3.4334 (3.6041)	Prec@1 53.906 (51.026)
Epoch: [0][250/10010]	Time 0.285 (0.293)	Data 0.001 (0.001)	Loss 3.4597 (3.5632)	Prec@1 48.438 (51.553)
Epoch: [0][300/10010]	Time 0.286 (0.292)	Data 0.001 (0.001)	Loss 3.2645 (3.5249)	Prec@1 55.469 (51.973)
Epoch: [0][350/10010]	Time 0.287 (0.291)	Data 0.001 (0.001)	Loss 3.5484 (3.4971)	Prec@1 47.656 (52.117)
Epoch: [0][400/10010]	Time 0.289 (0.290)	Data 0.001 (0.001)	Loss 3.2323 (3.4741)	Prec@1 56.250 (52.326)
Epoch: [0][450/10010]	Time 0.283 (0.289)	Data 0.001 (0.001)	Loss 3.4219 (3.4517)	Prec@1 45.312 (52.446)
Epoch: [0][500/10010]	Time 0.290 (0.289)	Data 0.000 (0.001)	Loss 3.0877 (3.4315)	Prec@1 53.906 (52.645)
Epoch: [0][550/10010]	Time 0.283 (0.288)	Data 0.001 (0.001)	Loss 2.8999 (3.4121)	Prec@1 64.062 (52.799)
Epoch: [0][600/10010]	Time 0.285 (0.288)	Data 0.001 (0.001)	Loss 3.1512 (3.3970)	Prec@1 55.469 (52.839)
Epoch: [0][650/10010]	Time 0.291 (0.288)	Data 0.001 (0.001)	Loss 3.4922 (3.3794)	Prec@1 41.406 (53.042)
Epoch: [0][700/10010]	Time 0.287 (0.288)	Data 0.001 (0.001)	Loss 3.2806 (3.3652)	Prec@1 54.688 (53.134)
Epoch: [0][750/10010]	Time 0.289 (0.288)	Data 0.001 (0.001)	Loss 3.0202 (3.3522)	Prec@1 59.375 (53.221)
Epoch: [0][800/10010]	Time 0.286 (0.287)	Data 0.001 (0.001)	Loss 3.2809 (3.3404)	Prec@1 46.875 (53.319)
Epoch: [0][850/10010]	Time 0.274 (0.287)	Data 0.001 (0.001)	Loss 3.3220 (3.3292)	Prec@1 48.438 (53.442)
Epoch: [0][900/10010]	Time 0.286 (0.287)	Data 0.001 (0.001)	Loss 3.1191 (3.3189)	Prec@1 53.906 (53.585)
Epoch: [0][950/10010]	Time 0.281 (0.287)	Data 0.001 (0.001)	Loss 3.0612 (3.3097)	Prec@1 53.125 (53.617)
Epoch: [0][1000/10010]	Time 0.290 (0.287)	Data 0.001 (0.001)	Loss 2.9329 (3.3006)	Prec@1 59.375 (53.692)
Epoch: [0][1050/10010]	Time 0.286 (0.287)	Data 0.001 (0.001)	Loss 3.1440 (3.2938)	Prec@1 49.219 (53.685)
Epoch: [0][1100/10010]	Time 0.288 (0.287)	Data 0.001 (0.001)	Loss 3.0438 (3.2841)	Prec@1 54.688 (53.801)
Epoch: [0][1150/10010]	Time 0.281 (0.286)	Data 0.001 (0.001)	Loss 3.0127 (3.2764)	Prec@1 57.031 (53.830)
Epoch: [0][1200/10010]	Time 0.269 (0.286)	Data 0.001 (0.001)	Loss 3.0933 (3.2686)	Prec@1 50.781 (53.881)
Epoch: [0][1250/10010]	Time 0.273 (0.286)	Data 0.001 (0.001)	Loss 3.1688 (3.2613)	Prec@1 48.438 (53.966)
Epoch: [0][1300/10010]	Time 0.290 (0.286)	Data 0.001 (0.001)	Loss 3.0718 (3.2533)	Prec@1 52.344 (54.052)
Epoch: [0][1350/10010]	Time 0.268 (0.286)	Data 0.001 (0.001)	Loss 2.9031 (3.2463)	Prec@1 61.719 (54.123)
Epoch: [0][1400/10010]	Time 0.288 (0.286)	Data 0.001 (0.001)	Loss 2.8733 (3.2403)	Prec@1 58.594 (54.181)
Epoch: [0][1450/10010]	Time 0.287 (0.286)	Data 0.001 (0.001)	Loss 3.1135 (3.2341)	Prec@1 55.469 (54.250)
Epoch: [0][1500/10010]	Time 0.285 (0.286)	Data 0.001 (0.001)	Loss 2.9855 (3.2280)	Prec@1 59.375 (54.314)
Epoch: [0][1550/10010]	Time 0.287 (0.286)	Data 0.001 (0.001)	Loss 3.2689 (3.2224)	Prec@1 50.000 (54.352)
Epoch: [0][1600/10010]	Time 0.287 (0.286)	Data 0.001 (0.001)	Loss 2.8151 (3.2166)	Prec@1 64.062 (54.413)
Epoch: [0][1650/10010]	Time 0.286 (0.286)	Data 0.001 (0.001)	Loss 2.8884 (3.2112)	Prec@1 63.281 (54.450)
Epoch: [0][1700/10010]	Time 0.288 (0.286)	Data 0.001 (0.001)	Loss 3.0930 (3.2055)	Prec@1 58.594 (54.538)
Epoch: [0][1750/10010]	Time 0.287 (0.286)	Data 0.001 (0.001)	Loss 3.0929 (3.2002)	Prec@1 54.688 (54.603)
Epoch: [0][1800/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8134 (3.1961)	Prec@1 60.938 (54.634)
Epoch: [0][1850/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 2.9165 (3.1919)	Prec@1 62.500 (54.671)
Epoch: [0][1900/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 3.1706 (3.1874)	Prec@1 50.781 (54.729)
Epoch: [0][1950/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 3.1409 (3.1829)	Prec@1 54.688 (54.773)
Epoch: [0][2000/10010]	Time 0.276 (0.285)	Data 0.001 (0.001)	Loss 3.1987 (3.1790)	Prec@1 53.906 (54.812)
Epoch: [0][2050/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 3.0094 (3.1744)	Prec@1 53.906 (54.857)
Epoch: [0][2100/10010]	Time 0.284 (0.285)	Data 0.001 (0.001)	Loss 2.9926 (3.1713)	Prec@1 57.812 (54.878)
Epoch: [0][2150/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.9145 (3.1677)	Prec@1 61.719 (54.916)
Epoch: [0][2200/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.9731 (3.1649)	Prec@1 57.031 (54.928)
Epoch: [0][2250/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 3.1396 (3.1623)	Prec@1 51.562 (54.927)
Epoch: [0][2300/10010]	Time 0.270 (0.285)	Data 0.001 (0.001)	Loss 3.0666 (3.1586)	Prec@1 53.906 (54.957)
Epoch: [0][2350/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 2.9069 (3.1551)	Prec@1 55.469 (54.995)
Epoch: [0][2400/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 3.0647 (3.1515)	Prec@1 53.906 (55.038)
Epoch: [0][2450/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 3.1259 (3.1490)	Prec@1 54.688 (55.063)
Epoch: [0][2500/10010]	Time 0.260 (0.285)	Data 0.001 (0.001)	Loss 3.0957 (3.1461)	Prec@1 54.688 (55.083)
Epoch: [0][2550/10010]	Time 0.282 (0.285)	Data 0.001 (0.001)	Loss 3.0884 (3.1432)	Prec@1 52.344 (55.110)
Epoch: [0][2600/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 3.1489 (3.1403)	Prec@1 55.469 (55.147)
Epoch: [0][2650/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 3.0780 (3.1377)	Prec@1 52.344 (55.167)
Epoch: [0][2700/10010]	Time 0.273 (0.285)	Data 0.001 (0.001)	Loss 2.7477 (3.1347)	Prec@1 62.500 (55.191)
Epoch: [0][2750/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 3.0941 (3.1326)	Prec@1 57.812 (55.199)
Epoch: [0][2800/10010]	Time 0.273 (0.285)	Data 0.001 (0.001)	Loss 3.0018 (3.1296)	Prec@1 49.219 (55.233)
Epoch: [0][2850/10010]	Time 0.290 (0.285)	Data 0.001 (0.001)	Loss 2.9188 (3.1267)	Prec@1 58.594 (55.262)
Epoch: [0][2900/10010]	Time 0.285 (0.285)	Data 0.001 (0.001)	Loss 3.1052 (3.1236)	Prec@1 52.344 (55.292)
Epoch: [0][2950/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.8424 (3.1212)	Prec@1 69.531 (55.322)
Epoch: [0][3000/10010]	Time 0.279 (0.285)	Data 0.001 (0.001)	Loss 2.9954 (3.1183)	Prec@1 54.688 (55.351)
Epoch: [0][3050/10010]	Time 0.260 (0.285)	Data 0.001 (0.001)	Loss 2.9693 (3.1164)	Prec@1 55.469 (55.356)
Epoch: [0][3100/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.5704 (3.1137)	Prec@1 64.062 (55.397)
Epoch: [0][3150/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 2.9859 (3.1110)	Prec@1 60.156 (55.421)
Epoch: [0][3200/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 3.0239 (3.1089)	Prec@1 58.594 (55.456)
Epoch: [0][3250/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 3.0273 (3.1069)	Prec@1 53.125 (55.469)
Epoch: [0][3300/10010]	Time 0.264 (0.285)	Data 0.001 (0.001)	Loss 2.8878 (3.1049)	Prec@1 64.844 (55.492)
Epoch: [0][3350/10010]	Time 0.287 (0.285)	Data 0.000 (0.001)	Loss 2.7474 (3.1029)	Prec@1 71.094 (55.519)
Epoch: [0][3400/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.9791 (3.1005)	Prec@1 52.344 (55.551)
Epoch: [0][3450/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.7442 (3.0981)	Prec@1 61.719 (55.591)
Epoch: [0][3500/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 2.9381 (3.0962)	Prec@1 59.375 (55.615)
Epoch: [0][3550/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8941 (3.0943)	Prec@1 59.375 (55.640)
Epoch: [0][3600/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.7799 (3.0922)	Prec@1 60.938 (55.666)
Epoch: [0][3650/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 2.8531 (3.0907)	Prec@1 58.594 (55.667)
Epoch: [0][3700/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 2.9600 (3.0885)	Prec@1 55.469 (55.696)
Epoch: [0][3750/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.8265 (3.0866)	Prec@1 63.281 (55.711)
Epoch: [0][3800/10010]	Time 0.290 (0.285)	Data 0.001 (0.001)	Loss 2.7445 (3.0845)	Prec@1 58.594 (55.747)
Epoch: [0][3850/10010]	Time 0.293 (0.285)	Data 0.001 (0.001)	Loss 3.0814 (3.0826)	Prec@1 54.688 (55.777)
Epoch: [0][3900/10010]	Time 0.261 (0.285)	Data 0.001 (0.001)	Loss 3.0665 (3.0806)	Prec@1 57.812 (55.799)
Epoch: [0][3950/10010]	Time 0.276 (0.285)	Data 0.001 (0.001)	Loss 2.6576 (3.0789)	Prec@1 61.719 (55.818)
Epoch: [0][4000/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 3.0097 (3.0774)	Prec@1 53.125 (55.831)
Epoch: [0][4050/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8271 (3.0758)	Prec@1 62.500 (55.849)
Epoch: [0][4100/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8971 (3.0741)	Prec@1 60.938 (55.863)
Epoch: [0][4150/10010]	Time 0.273 (0.285)	Data 0.001 (0.001)	Loss 2.8712 (3.0724)	Prec@1 57.812 (55.876)
Epoch: [0][4200/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 3.0027 (3.0709)	Prec@1 62.500 (55.898)
Epoch: [0][4250/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 2.9961 (3.0692)	Prec@1 57.031 (55.916)
Epoch: [0][4300/10010]	Time 0.283 (0.285)	Data 0.001 (0.001)	Loss 3.0800 (3.0671)	Prec@1 50.000 (55.947)
Epoch: [0][4350/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.9576 (3.0653)	Prec@1 56.250 (55.977)
Epoch: [0][4400/10010]	Time 0.279 (0.285)	Data 0.001 (0.001)	Loss 3.0040 (3.0635)	Prec@1 58.594 (55.998)
Epoch: [0][4450/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.9557 (3.0624)	Prec@1 56.250 (56.003)
Epoch: [0][4500/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 3.1096 (3.0611)	Prec@1 55.469 (56.016)
Epoch: [0][4550/10010]	Time 0.294 (0.285)	Data 0.001 (0.001)	Loss 3.3736 (3.0599)	Prec@1 43.750 (56.024)
Epoch: [0][4600/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 3.0989 (3.0581)	Prec@1 55.469 (56.050)
Epoch: [0][4650/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.9286 (3.0564)	Prec@1 55.469 (56.071)
Epoch: [0][4700/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.8375 (3.0547)	Prec@1 58.594 (56.085)
Epoch: [0][4750/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 2.7649 (3.0533)	Prec@1 64.844 (56.098)
Epoch: [0][4800/10010]	Time 0.294 (0.285)	Data 0.001 (0.001)	Loss 2.9562 (3.0519)	Prec@1 53.906 (56.116)
Epoch: [0][4850/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.8368 (3.0504)	Prec@1 61.719 (56.139)
Epoch: [0][4900/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.9293 (3.0491)	Prec@1 53.906 (56.150)
Epoch: [0][4950/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.7392 (3.0474)	Prec@1 62.500 (56.176)
Epoch: [0][5000/10010]	Time 0.293 (0.285)	Data 0.001 (0.001)	Loss 2.9880 (3.0463)	Prec@1 57.031 (56.188)
Epoch: [0][5050/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 3.3374 (3.0452)	Prec@1 54.688 (56.207)
Epoch: [0][5100/10010]	Time 0.268 (0.285)	Data 0.001 (0.001)	Loss 2.8163 (3.0440)	Prec@1 61.719 (56.225)
Epoch: [0][5150/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.8928 (3.0430)	Prec@1 60.156 (56.237)
Epoch: [0][5200/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8708 (3.0416)	Prec@1 63.281 (56.254)
Epoch: [0][5250/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 3.0687 (3.0406)	Prec@1 50.000 (56.261)
Epoch: [0][5300/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.7016 (3.0393)	Prec@1 60.938 (56.278)
Epoch: [0][5350/10010]	Time 0.276 (0.285)	Data 0.001 (0.001)	Loss 2.7518 (3.0379)	Prec@1 60.156 (56.294)
Epoch: [0][5400/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 3.1804 (3.0365)	Prec@1 46.094 (56.317)
Epoch: [0][5450/10010]	Time 0.300 (0.285)	Data 0.001 (0.001)	Loss 2.7223 (3.0352)	Prec@1 60.938 (56.335)
Epoch: [0][5500/10010]	Time 0.278 (0.285)	Data 0.001 (0.001)	Loss 2.9551 (3.0341)	Prec@1 57.031 (56.349)
Epoch: [0][5550/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 2.9071 (3.0328)	Prec@1 57.031 (56.373)
Epoch: [0][5600/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.7380 (3.0314)	Prec@1 60.156 (56.401)
Epoch: [0][5650/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 3.0869 (3.0303)	Prec@1 53.906 (56.417)
Epoch: [0][5700/10010]	Time 0.261 (0.285)	Data 0.001 (0.001)	Loss 2.6978 (3.0288)	Prec@1 57.812 (56.440)
Epoch: [0][5750/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8217 (3.0277)	Prec@1 57.812 (56.456)
Epoch: [0][5800/10010]	Time 0.293 (0.285)	Data 0.001 (0.001)	Loss 2.8720 (3.0262)	Prec@1 58.594 (56.482)
Epoch: [0][5850/10010]	Time 0.285 (0.285)	Data 0.001 (0.001)	Loss 2.9051 (3.0251)	Prec@1 62.500 (56.500)
Epoch: [0][5900/10010]	Time 0.290 (0.285)	Data 0.001 (0.001)	Loss 2.9244 (3.0241)	Prec@1 57.812 (56.509)
Epoch: [0][5950/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8495 (3.0231)	Prec@1 60.156 (56.524)
Epoch: [0][6000/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.8352 (3.0222)	Prec@1 60.156 (56.536)
Epoch: [0][6050/10010]	Time 0.261 (0.285)	Data 0.001 (0.001)	Loss 2.9559 (3.0212)	Prec@1 55.469 (56.552)
Epoch: [0][6100/10010]	Time 0.263 (0.285)	Data 0.002 (0.001)	Loss 2.8061 (3.0201)	Prec@1 62.500 (56.562)
Epoch: [0][6150/10010]	Time 0.263 (0.285)	Data 0.001 (0.001)	Loss 3.0229 (3.0191)	Prec@1 55.469 (56.574)
Epoch: [0][6200/10010]	Time 0.261 (0.285)	Data 0.001 (0.001)	Loss 2.6774 (3.0182)	Prec@1 66.406 (56.586)
Epoch: [0][6250/10010]	Time 0.261 (0.284)	Data 0.001 (0.001)	Loss 3.0330 (3.0171)	Prec@1 62.500 (56.601)
Epoch: [0][6300/10010]	Time 0.261 (0.284)	Data 0.001 (0.001)	Loss 2.9259 (3.0161)	Prec@1 55.469 (56.613)
Epoch: [0][6350/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.8825 (3.0153)	Prec@1 58.594 (56.620)
Epoch: [0][6400/10010]	Time 0.284 (0.284)	Data 0.001 (0.001)	Loss 2.6545 (3.0144)	Prec@1 61.719 (56.627)
Epoch: [0][6450/10010]	Time 0.289 (0.284)	Data 0.001 (0.001)	Loss 3.0638 (3.0134)	Prec@1 57.812 (56.640)
Epoch: [0][6500/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 3.0805 (3.0123)	Prec@1 50.000 (56.655)
Epoch: [0][6550/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.8135 (3.0114)	Prec@1 59.375 (56.667)
Epoch: [0][6600/10010]	Time 0.289 (0.284)	Data 0.001 (0.001)	Loss 2.9101 (3.0105)	Prec@1 58.594 (56.674)
Epoch: [0][6650/10010]	Time 0.297 (0.284)	Data 0.001 (0.001)	Loss 2.9781 (3.0097)	Prec@1 57.031 (56.686)
Epoch: [0][6700/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.6270 (3.0087)	Prec@1 64.062 (56.701)
Epoch: [0][6750/10010]	Time 0.268 (0.284)	Data 0.001 (0.001)	Loss 2.8682 (3.0076)	Prec@1 60.938 (56.715)
Epoch: [0][6800/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.8084 (3.0066)	Prec@1 58.594 (56.725)
Epoch: [0][6850/10010]	Time 0.265 (0.284)	Data 0.001 (0.001)	Loss 2.9137 (3.0056)	Prec@1 60.938 (56.739)
Epoch: [0][6900/10010]	Time 0.274 (0.284)	Data 0.001 (0.001)	Loss 2.8059 (3.0046)	Prec@1 58.594 (56.753)
Epoch: [0][6950/10010]	Time 0.261 (0.284)	Data 0.001 (0.001)	Loss 2.7985 (3.0038)	Prec@1 62.500 (56.763)
Epoch: [0][7000/10010]	Time 0.279 (0.284)	Data 0.001 (0.001)	Loss 2.9117 (3.0029)	Prec@1 55.469 (56.782)
Epoch: [0][7050/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.8938 (3.0018)	Prec@1 55.469 (56.796)
Epoch: [0][7100/10010]	Time 0.294 (0.284)	Data 0.001 (0.001)	Loss 2.9940 (3.0008)	Prec@1 51.562 (56.811)
Epoch: [0][7150/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.5675 (2.9999)	Prec@1 66.406 (56.825)
Epoch: [0][7200/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.9799 (2.9990)	Prec@1 57.031 (56.842)
Epoch: [0][7250/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 3.1080 (2.9982)	Prec@1 52.344 (56.852)
Epoch: [0][7300/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.8441 (2.9973)	Prec@1 63.281 (56.868)
Epoch: [0][7350/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.7969 (2.9965)	Prec@1 58.594 (56.881)
Epoch: [0][7400/10010]	Time 0.281 (0.284)	Data 0.001 (0.001)	Loss 2.8411 (2.9957)	Prec@1 57.812 (56.893)
Epoch: [0][7450/10010]	Time 0.280 (0.284)	Data 0.001 (0.001)	Loss 2.8920 (2.9949)	Prec@1 53.125 (56.905)
Epoch: [0][7500/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.8146 (2.9941)	Prec@1 61.719 (56.918)
Epoch: [0][7550/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.4511 (2.9934)	Prec@1 71.875 (56.923)
Epoch: [0][7600/10010]	Time 0.294 (0.284)	Data 0.001 (0.001)	Loss 2.6406 (2.9924)	Prec@1 65.625 (56.942)
Epoch: [0][7650/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.9660 (2.9919)	Prec@1 56.250 (56.943)
Epoch: [0][7700/10010]	Time 0.289 (0.284)	Data 0.001 (0.001)	Loss 2.7103 (2.9913)	Prec@1 61.719 (56.950)
Epoch: [0][7750/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 3.0718 (2.9905)	Prec@1 54.688 (56.961)
Epoch: [0][7800/10010]	Time 0.273 (0.284)	Data 0.001 (0.001)	Loss 3.0478 (2.9899)	Prec@1 53.906 (56.966)
Epoch: [0][7850/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 3.0702 (2.9892)	Prec@1 54.688 (56.977)
Epoch: [0][7900/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 3.1402 (2.9884)	Prec@1 50.000 (56.990)
Epoch: [0][7950/10010]	Time 0.287 (0.284)	Data 0.000 (0.001)	Loss 2.9921 (2.9876)	Prec@1 53.125 (57.004)
Epoch: [0][8000/10010]	Time 0.275 (0.284)	Data 0.001 (0.001)	Loss 3.0075 (2.9869)	Prec@1 53.125 (57.017)
Epoch: [0][8050/10010]	Time 0.286 (0.284)	Data 0.001 (0.001)	Loss 2.7988 (2.9861)	Prec@1 61.719 (57.027)
Epoch: [0][8100/10010]	Time 0.327 (0.284)	Data 0.042 (0.001)	Loss 2.8982 (2.9852)	Prec@1 57.812 (57.038)
Epoch: [0][8150/10010]	Time 0.294 (0.285)	Data 0.001 (0.001)	Loss 2.9869 (2.9844)	Prec@1 60.938 (57.053)
Epoch: [0][8200/10010]	Time 0.277 (0.285)	Data 0.001 (0.001)	Loss 2.9072 (2.9836)	Prec@1 58.594 (57.065)
Epoch: [0][8250/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 2.8060 (2.9829)	Prec@1 64.062 (57.079)
Epoch: [0][8300/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.7529 (2.9820)	Prec@1 59.375 (57.093)
Epoch: [0][8350/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 2.8094 (2.9813)	Prec@1 63.281 (57.102)
Epoch: [0][8400/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.6522 (2.9806)	Prec@1 67.188 (57.110)
Epoch: [0][8450/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.9554 (2.9798)	Prec@1 57.812 (57.124)
Epoch: [0][8500/10010]	Time 0.290 (0.285)	Data 0.001 (0.001)	Loss 2.9483 (2.9792)	Prec@1 59.375 (57.135)
Epoch: [0][8550/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.8927 (2.9786)	Prec@1 55.469 (57.144)
Epoch: [0][8600/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 2.7755 (2.9779)	Prec@1 57.812 (57.155)
Epoch: [0][8650/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 2.6495 (2.9771)	Prec@1 60.938 (57.164)
Epoch: [0][8700/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.6381 (2.9764)	Prec@1 63.281 (57.174)
Epoch: [0][8750/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 2.6888 (2.9757)	Prec@1 62.500 (57.185)
Epoch: [0][8800/10010]	Time 0.288 (0.285)	Data 0.000 (0.001)	Loss 2.8378 (2.9751)	Prec@1 68.750 (57.193)
Epoch: [0][8850/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.7734 (2.9744)	Prec@1 60.938 (57.204)
Epoch: [0][8900/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 3.0438 (2.9736)	Prec@1 53.906 (57.214)
Epoch: [0][8950/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.7389 (2.9731)	Prec@1 64.062 (57.222)
Epoch: [0][9000/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.6299 (2.9725)	Prec@1 61.719 (57.225)
Epoch: [0][9050/10010]	Time 0.281 (0.285)	Data 0.001 (0.001)	Loss 2.8801 (2.9719)	Prec@1 57.812 (57.232)
Epoch: [0][9100/10010]	Time 0.289 (0.285)	Data 0.001 (0.001)	Loss 2.9346 (2.9713)	Prec@1 58.594 (57.239)
Epoch: [0][9150/10010]	Time 0.278 (0.285)	Data 0.001 (0.001)	Loss 2.6145 (2.9707)	Prec@1 63.281 (57.250)
Epoch: [0][9200/10010]	Time 0.289 (0.285)	Data 0.000 (0.001)	Loss 3.0493 (2.9703)	Prec@1 54.688 (57.254)
Epoch: [0][9250/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.9244 (2.9697)	Prec@1 59.375 (57.266)
Epoch: [0][9300/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 2.8431 (2.9692)	Prec@1 63.281 (57.276)
Epoch: [0][9350/10010]	Time 0.273 (0.285)	Data 0.001 (0.001)	Loss 2.9969 (2.9684)	Prec@1 56.250 (57.288)
Epoch: [0][9400/10010]	Time 0.290 (0.285)	Data 0.001 (0.001)	Loss 2.8091 (2.9679)	Prec@1 57.031 (57.294)
Epoch: [0][9450/10010]	Time 0.288 (0.285)	Data 0.001 (0.001)	Loss 2.8138 (2.9673)	Prec@1 58.594 (57.306)
Epoch: [0][9500/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8182 (2.9667)	Prec@1 55.469 (57.309)
Epoch: [0][9550/10010]	Time 0.286 (0.285)	Data 0.000 (0.001)	Loss 3.0914 (2.9661)	Prec@1 50.000 (57.320)
Epoch: [0][9600/10010]	Time 0.278 (0.285)	Data 0.001 (0.001)	Loss 2.9403 (2.9656)	Prec@1 58.594 (57.326)
Epoch: [0][9650/10010]	Time 0.293 (0.285)	Data 0.001 (0.001)	Loss 2.8053 (2.9651)	Prec@1 59.375 (57.331)
Epoch: [0][9700/10010]	Time 0.294 (0.285)	Data 0.001 (0.001)	Loss 2.6362 (2.9645)	Prec@1 62.500 (57.340)
Epoch: [0][9750/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 2.7355 (2.9638)	Prec@1 62.500 (57.351)
Epoch: [0][9800/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.4961 (2.9632)	Prec@1 65.625 (57.361)
Epoch: [0][9850/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.9668 (2.9625)	Prec@1 53.125 (57.368)
Epoch: [0][9900/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8513 (2.9620)	Prec@1 57.812 (57.371)
Epoch: [0][9950/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8537 (2.9616)	Prec@1 55.469 (57.378)
Epoch: [0][10000/10010]	Time 0.283 (0.285)	Data 0.001 (0.001)	Loss 2.9592 (2.9610)	Prec@1 56.250 (57.385)
Test: [0/391]	Time 0.083 (0.083)	Loss 0.9724 (0.9724)	Prec@1 85.938 (85.938)
Test: [50/391]	Time 0.083 (0.079)	Loss 1.1892 (1.3407)	Prec@1 78.906 (76.302)
Test: [100/391]	Time 0.073 (0.078)	Loss 1.3734 (1.4362)	Prec@1 75.781 (72.316)
Test: [150/391]	Time 0.075 (0.079)	Loss 1.5879 (1.4394)	Prec@1 57.812 (72.351)
Test: [200/391]	Time 0.077 (0.079)	Loss 2.0384 (1.5844)	Prec@1 52.344 (69.244)
Test: [250/391]	Time 0.097 (0.079)	Loss 1.5492 (1.6676)	Prec@1 72.656 (67.530)
Test: [300/391]	Time 0.094 (0.078)	Loss 1.6715 (1.7417)	Prec@1 75.781 (65.913)
Test: [350/391]	Time 0.092 (0.078)	Loss 2.1370 (1.8010)	Prec@1 60.938 (64.666)
 * Prec@1 64.968
current lr 1.00000e-04
Epoch: [1][0/10010]	Time 0.288 (0.288)	Data 0.001 (0.001)	Loss 2.8673 (2.8673)	Prec@1 64.844 (64.844)
Epoch: [1][50/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.9841 (2.8381)	Prec@1 57.031 (60.034)
Epoch: [1][100/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.9519 (2.8342)	Prec@1 55.469 (60.110)
Epoch: [1][150/10010]	Time 0.285 (0.284)	Data 0.000 (0.001)	Loss 2.8561 (2.8360)	Prec@1 57.031 (59.804)
Epoch: [1][200/10010]	Time 0.269 (0.284)	Data 0.000 (0.001)	Loss 2.8372 (2.8339)	Prec@1 56.250 (59.565)
Epoch: [1][250/10010]	Time 0.261 (0.284)	Data 0.001 (0.001)	Loss 2.9180 (2.8307)	Prec@1 54.688 (59.627)
Epoch: [1][300/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.9844 (2.8353)	Prec@1 56.250 (59.536)
Epoch: [1][350/10010]	Time 0.289 (0.284)	Data 0.000 (0.001)	Loss 2.9311 (2.8383)	Prec@1 53.125 (59.522)
Epoch: [1][400/10010]	Time 0.271 (0.284)	Data 0.001 (0.001)	Loss 2.7971 (2.8412)	Prec@1 62.500 (59.496)
Epoch: [1][450/10010]	Time 0.286 (0.284)	Data 0.000 (0.001)	Loss 3.0407 (2.8428)	Prec@1 53.125 (59.330)
Epoch: [1][500/10010]	Time 0.285 (0.284)	Data 0.001 (0.001)	Loss 2.9447 (2.8440)	Prec@1 56.250 (59.277)
Epoch: [1][550/10010]	Time 0.285 (0.284)	Data 0.001 (0.001)	Loss 2.8330 (2.8458)	Prec@1 60.156 (59.181)
Epoch: [1][600/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.8512 (2.8472)	Prec@1 59.375 (59.122)
Epoch: [1][650/10010]	Time 0.286 (0.284)	Data 0.001 (0.001)	Loss 2.8160 (2.8453)	Prec@1 60.938 (59.203)
Epoch: [1][700/10010]	Time 0.283 (0.284)	Data 0.000 (0.001)	Loss 2.9815 (2.8464)	Prec@1 57.812 (59.151)
Epoch: [1][750/10010]	Time 0.285 (0.284)	Data 0.001 (0.001)	Loss 2.7774 (2.8465)	Prec@1 60.156 (59.142)
Epoch: [1][800/10010]	Time 0.268 (0.284)	Data 0.001 (0.001)	Loss 3.0135 (2.8454)	Prec@1 51.562 (59.157)
Epoch: [1][850/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 3.0980 (2.8456)	Prec@1 50.781 (59.135)
Epoch: [1][900/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.6226 (2.8452)	Prec@1 67.188 (59.150)
Epoch: [1][950/10010]	Time 0.271 (0.284)	Data 0.001 (0.001)	Loss 2.7291 (2.8451)	Prec@1 60.938 (59.169)
Epoch: [1][1000/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.7473 (2.8440)	Prec@1 53.906 (59.167)
Epoch: [1][1050/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.9734 (2.8448)	Prec@1 54.688 (59.136)
Epoch: [1][1100/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.9379 (2.8453)	Prec@1 62.500 (59.118)
Epoch: [1][1150/10010]	Time 0.271 (0.284)	Data 0.001 (0.001)	Loss 2.6009 (2.8449)	Prec@1 64.062 (59.120)
Epoch: [1][1200/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.9935 (2.8452)	Prec@1 56.250 (59.097)
Epoch: [1][1250/10010]	Time 0.289 (0.284)	Data 0.001 (0.001)	Loss 2.8850 (2.8446)	Prec@1 60.938 (59.138)
Epoch: [1][1300/10010]	Time 0.285 (0.284)	Data 0.001 (0.001)	Loss 2.6754 (2.8437)	Prec@1 60.938 (59.146)
Epoch: [1][1350/10010]	Time 0.274 (0.284)	Data 0.001 (0.001)	Loss 2.8692 (2.8441)	Prec@1 64.062 (59.129)
Epoch: [1][1400/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.7222 (2.8438)	Prec@1 65.625 (59.147)
Epoch: [1][1450/10010]	Time 0.285 (0.284)	Data 0.000 (0.001)	Loss 2.7531 (2.8434)	Prec@1 65.625 (59.152)
Epoch: [1][1500/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.8405 (2.8433)	Prec@1 59.375 (59.153)
Epoch: [1][1550/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.9255 (2.8431)	Prec@1 60.156 (59.147)
Epoch: [1][1600/10010]	Time 0.271 (0.284)	Data 0.001 (0.001)	Loss 2.9137 (2.8435)	Prec@1 59.375 (59.129)
Epoch: [1][1650/10010]	Time 0.286 (0.284)	Data 0.001 (0.001)	Loss 2.8273 (2.8435)	Prec@1 59.375 (59.141)
Epoch: [1][1700/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.9756 (2.8422)	Prec@1 55.469 (59.167)
Epoch: [1][1750/10010]	Time 0.286 (0.284)	Data 0.001 (0.001)	Loss 2.7611 (2.8422)	Prec@1 58.594 (59.160)
Epoch: [1][1800/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 3.0289 (2.8417)	Prec@1 55.469 (59.168)
Epoch: [1][1850/10010]	Time 0.286 (0.284)	Data 0.001 (0.001)	Loss 2.9848 (2.8417)	Prec@1 56.250 (59.156)
Epoch: [1][1900/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 3.0274 (2.8409)	Prec@1 50.000 (59.172)
Epoch: [1][1950/10010]	Time 0.285 (0.284)	Data 0.001 (0.001)	Loss 2.8578 (2.8406)	Prec@1 56.250 (59.159)
Epoch: [1][2000/10010]	Time 0.278 (0.284)	Data 0.001 (0.001)	Loss 2.8250 (2.8408)	Prec@1 64.062 (59.145)
Epoch: [1][2050/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.6810 (2.8404)	Prec@1 61.719 (59.159)
Epoch: [1][2100/10010]	Time 0.262 (0.284)	Data 0.001 (0.001)	Loss 2.8360 (2.8405)	Prec@1 53.906 (59.169)
Epoch: [1][2150/10010]	Time 0.289 (0.284)	Data 0.001 (0.001)	Loss 2.8968 (2.8404)	Prec@1 53.906 (59.177)
Epoch: [1][2200/10010]	Time 0.340 (0.285)	Data 0.057 (0.001)	Loss 2.9075 (2.8409)	Prec@1 55.469 (59.166)
Epoch: [1][2250/10010]	Time 0.260 (0.285)	Data 0.001 (0.001)	Loss 2.5993 (2.8414)	Prec@1 71.875 (59.141)
Epoch: [1][2300/10010]	Time 0.295 (0.285)	Data 0.001 (0.001)	Loss 2.9589 (2.8408)	Prec@1 54.688 (59.132)
Epoch: [1][2350/10010]	Time 0.286 (0.285)	Data 0.001 (0.001)	Loss 2.8146 (2.8407)	Prec@1 64.062 (59.135)
Epoch: [1][2400/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.8133 (2.8399)	Prec@1 57.812 (59.160)
Epoch: [1][2450/10010]	Time 0.276 (0.285)	Data 0.001 (0.001)	Loss 2.7104 (2.8398)	Prec@1 60.938 (59.156)
Epoch: [1][2500/10010]	Time 0.290 (0.285)	Data 0.001 (0.001)	Loss 2.8131 (2.8393)	Prec@1 60.156 (59.168)
Epoch: [1][2550/10010]	Time 0.266 (0.285)	Data 0.001 (0.001)	Loss 2.7202 (2.8393)	Prec@1 64.062 (59.177)
Epoch: [1][2600/10010]	Time 0.263 (0.285)	Data 0.001 (0.001)	Loss 2.8900 (2.8393)	Prec@1 60.156 (59.169)
Epoch: [1][2650/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.7719 (2.8391)	Prec@1 60.938 (59.173)
Epoch: [1][2700/10010]	Time 0.287 (0.285)	Data 0.001 (0.001)	Loss 2.9294 (2.8388)	Prec@1 56.250 (59.164)
Epoch: [1][2750/10010]	Time 0.285 (0.284)	Data 0.001 (0.001)	Loss 2.8129 (2.8393)	Prec@1 57.812 (59.146)
Epoch: [1][2800/10010]	Time 0.260 (0.284)	Data 0.001 (0.001)	Loss 2.5318 (2.8382)	Prec@1 69.531 (59.170)
Epoch: [1][2850/10010]	Time 0.286 (0.284)	Data 0.001 (0.001)	Loss 2.7331 (2.8380)	Prec@1 60.156 (59.182)
Epoch: [1][2900/10010]	Time 0.286 (0.284)	Data 0.001 (0.001)	Loss 2.7701 (2.8376)	Prec@1 57.031 (59.177)
Epoch: [1][2950/10010]	Time 0.278 (0.284)	Data 0.001 (0.001)	Loss 2.6806 (2.8373)	Prec@1 66.406 (59.183)
Epoch: [1][3000/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 3.1051 (2.8373)	Prec@1 57.031 (59.181)
Epoch: [1][3050/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.6931 (2.8376)	Prec@1 64.062 (59.177)
Epoch: [1][3100/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.8127 (2.8372)	Prec@1 61.719 (59.191)
Epoch: [1][3150/10010]	Time 0.260 (0.284)	Data 0.001 (0.001)	Loss 2.8819 (2.8370)	Prec@1 61.719 (59.185)
Epoch: [1][3200/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.9187 (2.8371)	Prec@1 53.906 (59.183)
Epoch: [1][3250/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.7700 (2.8369)	Prec@1 65.625 (59.185)
Epoch: [1][3300/10010]	Time 0.285 (0.284)	Data 0.001 (0.001)	Loss 3.0108 (2.8372)	Prec@1 52.344 (59.180)
Epoch: [1][3350/10010]	Time 0.277 (0.284)	Data 0.001 (0.001)	Loss 3.1086 (2.8373)	Prec@1 53.125 (59.185)
Epoch: [1][3400/10010]	Time 0.289 (0.284)	Data 0.001 (0.001)	Loss 2.9153 (2.8371)	Prec@1 53.125 (59.194)
Epoch: [1][3450/10010]	Time 0.289 (0.284)	Data 0.001 (0.001)	Loss 2.7994 (2.8368)	Prec@1 59.375 (59.204)
Epoch: [1][3500/10010]	Time 0.273 (0.284)	Data 0.001 (0.001)	Loss 2.6646 (2.8366)	Prec@1 67.969 (59.204)
Epoch: [1][3550/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.7343 (2.8366)	Prec@1 61.719 (59.199)
Epoch: [1][3600/10010]	Time 0.269 (0.284)	Data 0.001 (0.001)	Loss 2.7899 (2.8362)	Prec@1 59.375 (59.207)
Epoch: [1][3650/10010]	Time 0.269 (0.284)	Data 0.001 (0.001)	Loss 3.1044 (2.8367)	Prec@1 53.125 (59.191)
Epoch: [1][3700/10010]	Time 0.286 (0.284)	Data 0.001 (0.001)	Loss 2.8448 (2.8364)	Prec@1 62.500 (59.196)
Epoch: [1][3750/10010]	Time 0.261 (0.284)	Data 0.001 (0.001)	Loss 2.6902 (2.8363)	Prec@1 62.500 (59.189)
Epoch: [1][3800/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.9057 (2.8361)	Prec@1 54.688 (59.193)
Epoch: [1][3850/10010]	Time 0.260 (0.284)	Data 0.001 (0.001)	Loss 2.9247 (2.8358)	Prec@1 50.781 (59.207)
Epoch: [1][3900/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 3.0023 (2.8355)	Prec@1 63.281 (59.215)
Epoch: [1][3950/10010]	Time 0.267 (0.284)	Data 0.001 (0.001)	Loss 2.9640 (2.8356)	Prec@1 54.688 (59.213)
Epoch: [1][4000/10010]	Time 0.291 (0.284)	Data 0.001 (0.001)	Loss 2.6636 (2.8359)	Prec@1 60.156 (59.201)
Epoch: [1][4050/10010]	Time 0.286 (0.284)	Data 0.001 (0.001)	Loss 3.2442 (2.8363)	Prec@1 49.219 (59.196)
Epoch: [1][4100/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.8570 (2.8361)	Prec@1 60.156 (59.205)
Epoch: [1][4150/10010]	Time 0.288 (0.284)	Data 0.000 (0.001)	Loss 3.0665 (2.8360)	Prec@1 50.781 (59.204)
Epoch: [1][4200/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.7427 (2.8360)	Prec@1 64.062 (59.204)
Epoch: [1][4250/10010]	Time 0.276 (0.284)	Data 0.001 (0.001)	Loss 2.8351 (2.8362)	Prec@1 60.938 (59.212)
Epoch: [1][4300/10010]	Time 0.277 (0.284)	Data 0.001 (0.001)	Loss 2.9508 (2.8358)	Prec@1 58.594 (59.222)
Epoch: [1][4350/10010]	Time 0.288 (0.284)	Data 0.001 (0.001)	Loss 2.8241 (2.8354)	Prec@1 56.250 (59.235)
Epoch: [1][4400/10010]	Time 0.287 (0.284)	Data 0.001 (0.001)	Loss 2.8503 (2.8354)	Prec@1 58.594 (59.247)
Epoch: [1][4450/10010]	Time 0.290 (0.284)	Data 0.001 (0.001)	Loss 2.9301 (2.8356)	Prec@1 60.156 (59.236)
Traceback (most recent call last):
  File "trainer_imagenet_efficientnetv2s.py", line 458, in <module>
    main()
  File "trainer_imagenet_efficientnetv2s.py", line 272, in main
    train(train_loader, model, criterion_smooth, optimizer, epoch)
  File "trainer_imagenet_efficientnetv2s.py", line 310, in train
    for i, batch in enumerate(train_loader):
  File "/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/plugin/pytorch/__init__.py", line 180, in __next__
    outputs = self._get_outputs()
  File "/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/plugin/base_iterator.py", line 385, in _get_outputs
    outputs.append(p.share_outputs())
  File "/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/pipeline.py", line 1297, in share_outputs
    return self._pipe.ShareOutputs(types._raw_cuda_stream_ptr(cuda_stream))
RuntimeError: Critical error in pipeline:
Error in MIXED operator `nvidia.dali.ops.decoders.Image`,
which was used in the pipeline definition with the following traceback:

  File "/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/pipeline.py", line 819, in _build_graph
    outputs = define_graph()
  File "/ssd5/Roy/pytorch_resnet_cifar10-master/ImageNet_dali.py", line 24, in define_graph
    images = self.decode(self.jpegs)

encountered:

Error in thread 12: nvJPEG error (5): The user-provided allocator functions, for either memory allocation or for releasing the memory, returned a non-zero code.
Details:
n03447721/n03447721_43129.JPEG
Current pipeline object is no longer valid.
EfficientNet(
  (features): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
    (1): Sequential(
      (0): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
      (1): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.005, mode=row)
      )
    )
    (2): Sequential(
      (0): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.01, mode=row)
      )
      (1): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.015000000000000003, mode=row)
      )
      (2): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.02, mode=row)
      )
      (3): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.025, mode=row)
      )
    )
    (3): Sequential(
      (0): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.030000000000000006, mode=row)
      )
      (1): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.035, mode=row)
      )
      (2): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04, mode=row)
      )
      (3): FusedMBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.045, mode=row)
      )
    )
    (4): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05500000000000001, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.065, mode=row)
      )
      (4): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07, mode=row)
      )
      (5): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.075, mode=row)
      )
    )
    (5): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.085, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.095, mode=row)
      )
      (4): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1, mode=row)
      )
      (5): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10500000000000001, mode=row)
      )
      (6): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11000000000000001, mode=row)
      )
      (7): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11500000000000002, mode=row)
      )
      (8): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)
      )
    )
    (6): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.125, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.135, mode=row)
      )
    )
    (7): Conv2dNormActivation(
      (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
DALI "gpu" variant
pip_train.epoch_size("Reader"):1281167
/home/hsujenlung/miniconda3/envs/Roy/lib/python3.8/site-packages/nvidia/dali/plugin/base_iterator.py:208: Warning: Please set `reader_name` and don't set last_batch_padded and size manually whenever possible. This may lead, in some situations, to missing some samples or returning duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
current lr 1.00000e-04
Epoch: [0][0/12812]	Time 8.668 (8.668)	Data 0.014 (0.014)	Loss 3.7030 (3.7030)	Prec@1 54.000 (54.000)
Epoch: [0][50/12812]	Time 0.199 (0.366)	Data 0.001 (0.001)	Loss 3.6981 (3.7926)	Prec@1 51.000 (48.725)
Epoch: [0][100/12812]	Time 0.200 (0.284)	Data 0.001 (0.001)	Loss 3.4703 (3.7317)	Prec@1 51.000 (49.307)
Epoch: [0][150/12812]	Time 0.200 (0.257)	Data 0.001 (0.001)	Loss 3.3477 (3.6700)	Prec@1 53.000 (50.205)
Epoch: [0][200/12812]	Time 0.201 (0.243)	Data 0.001 (0.001)	Loss 3.5334 (3.6259)	Prec@1 49.000 (50.652)
Epoch: [0][250/12812]	Time 0.201 (0.234)	Data 0.001 (0.001)	Loss 3.7169 (3.5780)	Prec@1 48.000 (51.195)
Epoch: [0][300/12812]	Time 0.201 (0.229)	Data 0.001 (0.001)	Loss 3.3548 (3.5442)	Prec@1 57.000 (51.611)
Epoch: [0][350/12812]	Time 0.201 (0.225)	Data 0.001 (0.001)	Loss 3.4251 (3.5147)	Prec@1 51.000 (51.957)
Epoch: [0][400/12812]	Time 0.201 (0.222)	Data 0.001 (0.001)	Loss 3.3445 (3.4863)	Prec@1 48.000 (52.272)
Epoch: [0][450/12812]	Time 0.203 (0.220)	Data 0.001 (0.001)	Loss 3.4537 (3.4637)	Prec@1 51.000 (52.364)
Epoch: [0][500/12812]	Time 0.202 (0.218)	Data 0.001 (0.001)	Loss 3.2292 (3.4450)	Prec@1 51.000 (52.627)
Epoch: [0][550/12812]	Time 0.207 (0.217)	Data 0.001 (0.001)	Loss 3.5613 (3.4278)	Prec@1 43.000 (52.655)
Epoch: [0][600/12812]	Time 0.202 (0.216)	Data 0.001 (0.001)	Loss 3.3742 (3.4128)	Prec@1 52.000 (52.667)
Epoch: [0][650/12812]	Time 0.202 (0.215)	Data 0.001 (0.001)	Loss 3.3409 (3.3965)	Prec@1 53.000 (52.802)
Epoch: [0][700/12812]	Time 0.203 (0.214)	Data 0.001 (0.001)	Loss 3.0828 (3.3828)	Prec@1 59.000 (52.903)
Epoch: [0][750/12812]	Time 0.203 (0.213)	Data 0.001 (0.001)	Loss 3.3220 (3.3714)	Prec@1 49.000 (52.971)
Epoch: [0][800/12812]	Time 0.203 (0.212)	Data 0.001 (0.001)	Loss 3.2586 (3.3584)	Prec@1 53.000 (53.139)
Epoch: [0][850/12812]	Time 0.202 (0.212)	Data 0.001 (0.001)	Loss 3.0438 (3.3445)	Prec@1 64.000 (53.307)
Epoch: [0][900/12812]	Time 0.203 (0.211)	Data 0.001 (0.001)	Loss 3.1650 (3.3346)	Prec@1 49.000 (53.365)
Epoch: [0][950/12812]	Time 0.203 (0.211)	Data 0.001 (0.001)	Loss 3.0927 (3.3253)	Prec@1 56.000 (53.432)
Epoch: [0][1000/12812]	Time 0.202 (0.210)	Data 0.000 (0.001)	Loss 3.2147 (3.3159)	Prec@1 50.000 (53.511)
Epoch: [0][1050/12812]	Time 0.202 (0.210)	Data 0.001 (0.001)	Loss 3.0601 (3.3070)	Prec@1 54.000 (53.573)
Epoch: [0][1100/12812]	Time 0.203 (0.210)	Data 0.001 (0.001)	Loss 3.1609 (3.2996)	Prec@1 53.000 (53.608)
Epoch: [0][1150/12812]	Time 0.203 (0.209)	Data 0.001 (0.001)	Loss 3.1919 (3.2914)	Prec@1 57.000 (53.700)
Epoch: [0][1200/12812]	Time 0.203 (0.209)	Data 0.001 (0.001)	Loss 2.9332 (3.2844)	Prec@1 61.000 (53.731)
Epoch: [0][1250/12812]	Time 0.203 (0.209)	Data 0.001 (0.001)	Loss 2.9289 (3.2777)	Prec@1 60.000 (53.735)
Epoch: [0][1300/12812]	Time 0.203 (0.209)	Data 0.001 (0.001)	Loss 3.2172 (3.2702)	Prec@1 54.000 (53.792)
Epoch: [0][1350/12812]	Time 0.203 (0.208)	Data 0.001 (0.001)	Loss 3.1639 (3.2647)	Prec@1 52.000 (53.795)
Epoch: [0][1400/12812]	Time 0.203 (0.208)	Data 0.001 (0.001)	Loss 2.9961 (3.2581)	Prec@1 56.000 (53.892)
Epoch: [0][1450/12812]	Time 0.203 (0.208)	Data 0.001 (0.001)	Loss 2.9089 (3.2532)	Prec@1 58.000 (53.902)
Epoch: [0][1500/12812]	Time 0.203 (0.208)	Data 0.001 (0.001)	Loss 3.1315 (3.2464)	Prec@1 54.000 (53.976)
Epoch: [0][1550/12812]	Time 0.203 (0.208)	Data 0.001 (0.001)	Loss 2.8938 (3.2404)	Prec@1 60.000 (54.023)
Epoch: [0][1600/12812]	Time 0.203 (0.208)	Data 0.001 (0.001)	Loss 3.1821 (3.2351)	Prec@1 54.000 (54.092)
Epoch: [0][1650/12812]	Time 0.203 (0.207)	Data 0.001 (0.001)	Loss 2.9628 (3.2282)	Prec@1 54.000 (54.180)
Epoch: [0][1700/12812]	Time 0.202 (0.207)	Data 0.001 (0.001)	Loss 3.2499 (3.2236)	Prec@1 53.000 (54.205)
Epoch: [0][1750/12812]	Time 0.203 (0.207)	Data 0.001 (0.001)	Loss 2.9844 (3.2186)	Prec@1 59.000 (54.256)
Epoch: [0][1800/12812]	Time 0.204 (0.207)	Data 0.001 (0.001)	Loss 2.9539 (3.2145)	Prec@1 53.000 (54.290)
Epoch: [0][1850/12812]	Time 0.202 (0.207)	Data 0.001 (0.001)	Loss 3.2998 (3.2105)	Prec@1 53.000 (54.312)
Epoch: [0][1900/12812]	Time 0.202 (0.207)	Data 0.001 (0.001)	Loss 3.1759 (3.2056)	Prec@1 56.000 (54.336)
Epoch: [0][1950/12812]	Time 0.203 (0.207)	Data 0.001 (0.001)	Loss 3.1084 (3.2011)	Prec@1 56.000 (54.382)
Epoch: [0][2000/12812]	Time 0.204 (0.207)	Data 0.001 (0.001)	Loss 2.7836 (3.1973)	Prec@1 57.000 (54.427)
Epoch: [0][2050/12812]	Time 0.203 (0.207)	Data 0.001 (0.001)	Loss 2.9513 (3.1939)	Prec@1 52.000 (54.423)
Epoch: [0][2100/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 3.1806 (3.1897)	Prec@1 58.000 (54.466)
Epoch: [0][2150/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 2.9377 (3.1856)	Prec@1 60.000 (54.524)
Epoch: [0][2200/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 2.8328 (3.1818)	Prec@1 59.000 (54.574)
Epoch: [0][2250/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 2.9626 (3.1780)	Prec@1 55.000 (54.628)
Epoch: [0][2300/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 3.0083 (3.1751)	Prec@1 54.000 (54.655)
Epoch: [0][2350/12812]	Time 0.202 (0.206)	Data 0.001 (0.001)	Loss 3.2074 (3.1720)	Prec@1 50.000 (54.687)
Epoch: [0][2400/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 3.0714 (3.1682)	Prec@1 51.000 (54.733)
Epoch: [0][2450/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 3.0776 (3.1655)	Prec@1 55.000 (54.733)
Epoch: [0][2500/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 3.0826 (3.1626)	Prec@1 53.000 (54.762)
Epoch: [0][2550/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 2.9957 (3.1601)	Prec@1 50.000 (54.783)
Epoch: [0][2600/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 3.0176 (3.1571)	Prec@1 49.000 (54.822)
Epoch: [0][2650/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 2.9302 (3.1534)	Prec@1 55.000 (54.872)
Epoch: [0][2700/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 3.3119 (3.1500)	Prec@1 48.000 (54.919)
Epoch: [0][2750/12812]	Time 0.204 (0.206)	Data 0.001 (0.001)	Loss 3.1650 (3.1470)	Prec@1 52.000 (54.955)
Epoch: [0][2800/12812]	Time 0.202 (0.206)	Data 0.001 (0.001)	Loss 3.2144 (3.1450)	Prec@1 53.000 (54.959)
Epoch: [0][2850/12812]	Time 0.203 (0.206)	Data 0.001 (0.001)	Loss 3.1046 (3.1426)	Prec@1 56.000 (54.975)
Epoch: [0][2900/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.7551 (3.1406)	Prec@1 63.000 (54.982)
Epoch: [0][2950/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.9248 (3.1377)	Prec@1 59.000 (55.025)
Epoch: [0][3000/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.7911 (3.1350)	Prec@1 62.000 (55.043)
Epoch: [0][3050/12812]	Time 0.204 (0.205)	Data 0.001 (0.001)	Loss 2.9732 (3.1322)	Prec@1 65.000 (55.101)
Epoch: [0][3100/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.1229 (3.1301)	Prec@1 57.000 (55.114)
Epoch: [0][3150/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.9453 (3.1279)	Prec@1 55.000 (55.160)
Epoch: [0][3200/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.8787 (3.1253)	Prec@1 63.000 (55.201)
Epoch: [0][3250/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.0450 (3.1232)	Prec@1 54.000 (55.240)
Epoch: [0][3300/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.0895 (3.1209)	Prec@1 48.000 (55.267)
Epoch: [0][3350/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.8489 (3.1190)	Prec@1 59.000 (55.276)
Epoch: [0][3400/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.9836 (3.1167)	Prec@1 53.000 (55.290)
Epoch: [0][3450/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.5554 (3.1143)	Prec@1 65.000 (55.317)
Epoch: [0][3500/12812]	Time 0.204 (0.205)	Data 0.001 (0.001)	Loss 3.0108 (3.1126)	Prec@1 54.000 (55.316)
Epoch: [0][3550/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.8418 (3.1105)	Prec@1 60.000 (55.338)
Epoch: [0][3600/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.9661 (3.1081)	Prec@1 60.000 (55.375)
Epoch: [0][3650/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.8486 (3.1058)	Prec@1 53.000 (55.398)
Epoch: [0][3700/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.1384 (3.1031)	Prec@1 52.000 (55.430)
Epoch: [0][3750/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.8819 (3.1015)	Prec@1 61.000 (55.436)
Epoch: [0][3800/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.0935 (3.0995)	Prec@1 57.000 (55.463)
Epoch: [0][3850/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.0610 (3.0977)	Prec@1 58.000 (55.479)
Epoch: [0][3900/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.0461 (3.0961)	Prec@1 51.000 (55.501)
Epoch: [0][3950/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.0196 (3.0944)	Prec@1 55.000 (55.518)
Epoch: [0][4000/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.0321 (3.0922)	Prec@1 56.000 (55.542)
Epoch: [0][4050/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.0021 (3.0907)	Prec@1 55.000 (55.547)
Epoch: [0][4100/12812]	Time 0.202 (0.205)	Data 0.001 (0.001)	Loss 2.7921 (3.0890)	Prec@1 61.000 (55.564)
Epoch: [0][4150/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.0004 (3.0875)	Prec@1 56.000 (55.583)
Epoch: [0][4200/12812]	Time 0.202 (0.205)	Data 0.001 (0.001)	Loss 3.1216 (3.0859)	Prec@1 52.000 (55.595)
Epoch: [0][4250/12812]	Time 0.202 (0.205)	Data 0.001 (0.001)	Loss 2.9430 (3.0842)	Prec@1 58.000 (55.629)
Epoch: [0][4300/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.8787 (3.0828)	Prec@1 55.000 (55.636)
Epoch: [0][4350/12812]	Time 0.202 (0.205)	Data 0.001 (0.001)	Loss 2.9291 (3.0809)	Prec@1 61.000 (55.665)
Epoch: [0][4400/12812]	Time 0.202 (0.205)	Data 0.001 (0.001)	Loss 2.7076 (3.0794)	Prec@1 64.000 (55.690)
Epoch: [0][4450/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.7936 (3.0773)	Prec@1 59.000 (55.719)
Epoch: [0][4500/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.8344 (3.0759)	Prec@1 64.000 (55.734)
Epoch: [0][4550/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.8308 (3.0743)	Prec@1 59.000 (55.755)
Epoch: [0][4600/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.1068 (3.0725)	Prec@1 55.000 (55.784)
Epoch: [0][4650/12812]	Time 0.204 (0.205)	Data 0.001 (0.001)	Loss 3.0757 (3.0715)	Prec@1 52.000 (55.782)
Epoch: [0][4700/12812]	Time 0.204 (0.205)	Data 0.001 (0.001)	Loss 2.7268 (3.0702)	Prec@1 64.000 (55.800)
Epoch: [0][4750/12812]	Time 0.205 (0.205)	Data 0.001 (0.001)	Loss 2.9443 (3.0686)	Prec@1 56.000 (55.823)
Epoch: [0][4800/12812]	Time 0.202 (0.205)	Data 0.001 (0.001)	Loss 2.8767 (3.0670)	Prec@1 55.000 (55.835)
Epoch: [0][4850/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 3.0166 (3.0656)	Prec@1 60.000 (55.853)
Epoch: [0][4900/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.7747 (3.0641)	Prec@1 61.000 (55.874)
Epoch: [0][4950/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.8605 (3.0627)	Prec@1 63.000 (55.891)
Epoch: [0][5000/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.9623 (3.0614)	Prec@1 54.000 (55.914)
Epoch: [0][5050/12812]	Time 0.202 (0.205)	Data 0.001 (0.001)	Loss 3.0055 (3.0599)	Prec@1 51.000 (55.933)
Epoch: [0][5100/12812]	Time 0.202 (0.205)	Data 0.001 (0.001)	Loss 2.9713 (3.0588)	Prec@1 58.000 (55.941)
Epoch: [0][5150/12812]	Time 0.204 (0.205)	Data 0.001 (0.001)	Loss 2.9299 (3.0576)	Prec@1 57.000 (55.956)
Epoch: [0][5200/12812]	Time 0.202 (0.205)	Data 0.001 (0.001)	Loss 2.7297 (3.0565)	Prec@1 62.000 (55.974)
Epoch: [0][5250/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.7510 (3.0551)	Prec@1 60.000 (55.992)
Epoch: [0][5300/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.9644 (3.0538)	Prec@1 51.000 (56.007)
Epoch: [0][5350/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.8836 (3.0526)	Prec@1 58.000 (56.018)
Epoch: [0][5400/12812]	Time 0.203 (0.205)	Data 0.001 (0.001)	Loss 2.7923 (3.0518)	Prec@1 64.000 (56.028)
Epoch: [0][5450/12812]	Time 0.202 (0.205)	Data 0.001 (0.001)	Loss 2.9920 (3.0505)	Prec@1 62.000 (56.048)
Epoch: [0][5500/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8332 (3.0491)	Prec@1 54.000 (56.066)
Epoch: [0][5550/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0437 (3.0480)	Prec@1 55.000 (56.081)
Epoch: [0][5600/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9127 (3.0465)	Prec@1 58.000 (56.108)
Epoch: [0][5650/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9357 (3.0454)	Prec@1 56.000 (56.125)
Epoch: [0][5700/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8771 (3.0443)	Prec@1 63.000 (56.139)
Epoch: [0][5750/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9340 (3.0432)	Prec@1 53.000 (56.150)
Epoch: [0][5800/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0605 (3.0423)	Prec@1 47.000 (56.160)
Epoch: [0][5850/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 3.2397 (3.0412)	Prec@1 47.000 (56.169)
Epoch: [0][5900/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9996 (3.0399)	Prec@1 56.000 (56.191)
Epoch: [0][5950/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7801 (3.0390)	Prec@1 63.000 (56.207)
Epoch: [0][6000/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9360 (3.0379)	Prec@1 59.000 (56.223)
Epoch: [0][6050/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9533 (3.0367)	Prec@1 59.000 (56.238)
Epoch: [0][6100/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.7197 (3.0356)	Prec@1 65.000 (56.259)
Epoch: [0][6150/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9819 (3.0346)	Prec@1 61.000 (56.266)
Epoch: [0][6200/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 3.0350 (3.0335)	Prec@1 54.000 (56.284)
Epoch: [0][6250/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0042 (3.0321)	Prec@1 50.000 (56.301)
Epoch: [0][6300/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9465 (3.0312)	Prec@1 55.000 (56.314)
Epoch: [0][6350/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7970 (3.0302)	Prec@1 64.000 (56.328)
Epoch: [0][6400/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8848 (3.0292)	Prec@1 55.000 (56.337)
Epoch: [0][6450/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.7947 (3.0282)	Prec@1 66.000 (56.353)
Epoch: [0][6500/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 3.1823 (3.0273)	Prec@1 56.000 (56.373)
Epoch: [0][6550/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8826 (3.0263)	Prec@1 62.000 (56.387)
Epoch: [0][6600/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8961 (3.0255)	Prec@1 55.000 (56.395)
Epoch: [0][6650/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8533 (3.0247)	Prec@1 58.000 (56.404)
Epoch: [0][6700/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.1021 (3.0239)	Prec@1 48.000 (56.404)
Epoch: [0][6750/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.6177 (3.0231)	Prec@1 71.000 (56.411)
Epoch: [0][6800/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 3.0349 (3.0221)	Prec@1 55.000 (56.423)
Epoch: [0][6850/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0837 (3.0213)	Prec@1 50.000 (56.429)
Epoch: [0][6900/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8563 (3.0204)	Prec@1 63.000 (56.442)
Epoch: [0][6950/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0266 (3.0193)	Prec@1 51.000 (56.461)
Epoch: [0][7000/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.1069 (3.0186)	Prec@1 55.000 (56.467)
Epoch: [0][7050/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8633 (3.0177)	Prec@1 60.000 (56.478)
Epoch: [0][7100/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9677 (3.0171)	Prec@1 58.000 (56.486)
Epoch: [0][7150/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7244 (3.0160)	Prec@1 64.000 (56.508)
Epoch: [0][7200/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9657 (3.0150)	Prec@1 56.000 (56.526)
Epoch: [0][7250/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9058 (3.0140)	Prec@1 64.000 (56.543)
Epoch: [0][7300/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9171 (3.0129)	Prec@1 60.000 (56.565)
Epoch: [0][7350/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.8478 (3.0122)	Prec@1 57.000 (56.573)
Epoch: [0][7400/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8960 (3.0113)	Prec@1 58.000 (56.587)
Epoch: [0][7450/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8299 (3.0103)	Prec@1 64.000 (56.602)
Epoch: [0][7500/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9362 (3.0094)	Prec@1 57.000 (56.618)
Epoch: [0][7550/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9849 (3.0087)	Prec@1 53.000 (56.624)
Epoch: [0][7600/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 3.0709 (3.0080)	Prec@1 55.000 (56.630)
Epoch: [0][7650/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7200 (3.0073)	Prec@1 74.000 (56.646)
Epoch: [0][7700/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7134 (3.0065)	Prec@1 61.000 (56.654)
Epoch: [0][7750/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.1169 (3.0059)	Prec@1 52.000 (56.663)
Epoch: [0][7800/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.6282 (3.0052)	Prec@1 67.000 (56.675)
Epoch: [0][7850/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8008 (3.0043)	Prec@1 61.000 (56.690)
Epoch: [0][7900/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 3.3808 (3.0037)	Prec@1 50.000 (56.698)
Epoch: [0][7950/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8518 (3.0028)	Prec@1 58.000 (56.711)
Epoch: [0][8000/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9958 (3.0018)	Prec@1 55.000 (56.728)
Epoch: [0][8050/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.6894 (3.0010)	Prec@1 63.000 (56.739)
Epoch: [0][8100/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 3.0034 (3.0002)	Prec@1 50.000 (56.751)
Epoch: [0][8150/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8272 (2.9997)	Prec@1 64.000 (56.760)
Epoch: [0][8200/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.5182 (2.9990)	Prec@1 68.000 (56.768)
Epoch: [0][8250/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9100 (2.9982)	Prec@1 58.000 (56.779)
Epoch: [0][8300/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 3.0008 (2.9974)	Prec@1 53.000 (56.791)
Epoch: [0][8350/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.6128 (2.9967)	Prec@1 67.000 (56.795)
Epoch: [0][8400/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.8927 (2.9961)	Prec@1 54.000 (56.802)
Epoch: [0][8450/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9706 (2.9957)	Prec@1 53.000 (56.805)
Epoch: [0][8500/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8999 (2.9950)	Prec@1 58.000 (56.818)
Epoch: [0][8550/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9690 (2.9944)	Prec@1 61.000 (56.821)
Epoch: [0][8600/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.6885 (2.9937)	Prec@1 63.000 (56.830)
Epoch: [0][8650/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0469 (2.9929)	Prec@1 56.000 (56.843)
Epoch: [0][8700/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8445 (2.9923)	Prec@1 60.000 (56.855)
Epoch: [0][8750/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8079 (2.9915)	Prec@1 60.000 (56.864)
Epoch: [0][8800/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8315 (2.9909)	Prec@1 55.000 (56.875)
Epoch: [0][8850/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8541 (2.9901)	Prec@1 61.000 (56.887)
Epoch: [0][8900/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8442 (2.9896)	Prec@1 60.000 (56.897)
Epoch: [0][8950/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.6957 (2.9889)	Prec@1 65.000 (56.910)
Epoch: [0][9000/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7788 (2.9881)	Prec@1 59.000 (56.920)
Epoch: [0][9050/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0402 (2.9875)	Prec@1 50.000 (56.925)
Epoch: [0][9100/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8829 (2.9869)	Prec@1 58.000 (56.934)
Epoch: [0][9150/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9233 (2.9861)	Prec@1 64.000 (56.947)
Epoch: [0][9200/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 3.0635 (2.9855)	Prec@1 57.000 (56.957)
Epoch: [0][9250/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.4945 (2.9847)	Prec@1 65.000 (56.969)
Epoch: [0][9300/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.9118 (2.9841)	Prec@1 62.000 (56.977)
Epoch: [0][9350/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8785 (2.9834)	Prec@1 53.000 (56.990)
Epoch: [0][9400/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8783 (2.9827)	Prec@1 54.000 (57.001)
Epoch: [0][9450/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.7687 (2.9821)	Prec@1 54.000 (57.008)
Epoch: [0][9500/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.9016 (2.9813)	Prec@1 54.000 (57.023)
Epoch: [0][9550/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8996 (2.9807)	Prec@1 56.000 (57.036)
Epoch: [0][9600/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9418 (2.9800)	Prec@1 57.000 (57.043)
Epoch: [0][9650/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 3.0019 (2.9794)	Prec@1 51.000 (57.048)
Epoch: [0][9700/12812]	Time 0.202 (0.204)	Data 0.000 (0.001)	Loss 2.8515 (2.9787)	Prec@1 58.000 (57.064)
Epoch: [0][9750/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8489 (2.9781)	Prec@1 55.000 (57.071)
Epoch: [0][9800/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.1822 (2.9777)	Prec@1 52.000 (57.073)
Epoch: [0][9850/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0997 (2.9772)	Prec@1 53.000 (57.082)
Epoch: [0][9900/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8239 (2.9766)	Prec@1 56.000 (57.090)
Epoch: [0][9950/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8422 (2.9761)	Prec@1 57.000 (57.093)
Epoch: [0][10000/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9288 (2.9757)	Prec@1 56.000 (57.100)
Epoch: [0][10050/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7817 (2.9751)	Prec@1 64.000 (57.105)
Epoch: [0][10100/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8438 (2.9745)	Prec@1 58.000 (57.114)
Epoch: [0][10150/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.6810 (2.9740)	Prec@1 67.000 (57.125)
Epoch: [0][10200/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.8902 (2.9735)	Prec@1 60.000 (57.133)
Epoch: [0][10250/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0409 (2.9728)	Prec@1 58.000 (57.146)
Epoch: [0][10300/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7539 (2.9722)	Prec@1 65.000 (57.155)
Epoch: [0][10350/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8613 (2.9717)	Prec@1 53.000 (57.161)
Epoch: [0][10400/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8697 (2.9709)	Prec@1 57.000 (57.173)
Epoch: [0][10450/12812]	Time 0.202 (0.204)	Data 0.001 (0.001)	Loss 2.7828 (2.9704)	Prec@1 58.000 (57.181)
Epoch: [0][10500/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7510 (2.9697)	Prec@1 65.000 (57.191)
Epoch: [0][10550/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0545 (2.9691)	Prec@1 54.000 (57.200)
Epoch: [0][10600/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9539 (2.9687)	Prec@1 52.000 (57.206)
Epoch: [0][10650/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.8342 (2.9681)	Prec@1 58.000 (57.212)
Epoch: [0][10700/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.1272 (2.9676)	Prec@1 52.000 (57.222)
Epoch: [0][10750/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7530 (2.9671)	Prec@1 62.000 (57.225)
Epoch: [0][10800/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8868 (2.9665)	Prec@1 57.000 (57.236)
Epoch: [0][10850/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.8829 (2.9660)	Prec@1 54.000 (57.243)
Epoch: [0][10900/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7640 (2.9655)	Prec@1 63.000 (57.250)
Epoch: [0][10950/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7745 (2.9649)	Prec@1 66.000 (57.258)
Epoch: [0][11000/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9964 (2.9644)	Prec@1 57.000 (57.269)
Epoch: [0][11050/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7009 (2.9640)	Prec@1 66.000 (57.272)
Epoch: [0][11100/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8920 (2.9633)	Prec@1 56.000 (57.284)
Epoch: [0][11150/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0549 (2.9629)	Prec@1 61.000 (57.293)
Epoch: [0][11200/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8018 (2.9624)	Prec@1 56.000 (57.303)
Epoch: [0][11250/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8649 (2.9622)	Prec@1 59.000 (57.305)
Epoch: [0][11300/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7505 (2.9617)	Prec@1 60.000 (57.314)
Epoch: [0][11350/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9129 (2.9613)	Prec@1 59.000 (57.325)
Epoch: [0][11400/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9363 (2.9607)	Prec@1 53.000 (57.339)
Epoch: [0][11450/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.9695 (2.9601)	Prec@1 55.000 (57.348)
Epoch: [0][11500/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.7932 (2.9597)	Prec@1 57.000 (57.354)
Epoch: [0][11550/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0213 (2.9591)	Prec@1 52.000 (57.361)
Epoch: [0][11600/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8907 (2.9587)	Prec@1 63.000 (57.364)
Epoch: [0][11650/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9462 (2.9582)	Prec@1 53.000 (57.372)
Epoch: [0][11700/12812]	Time 0.203 (0.204)	Data 0.000 (0.001)	Loss 2.8234 (2.9577)	Prec@1 52.000 (57.382)
Epoch: [0][11750/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8062 (2.9573)	Prec@1 62.000 (57.391)
Epoch: [0][11800/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8351 (2.9568)	Prec@1 57.000 (57.402)
Epoch: [0][11850/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7535 (2.9563)	Prec@1 62.000 (57.410)
Epoch: [0][11900/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0351 (2.9558)	Prec@1 54.000 (57.416)
Epoch: [0][11950/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8083 (2.9552)	Prec@1 66.000 (57.428)
Epoch: [0][12000/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0153 (2.9549)	Prec@1 54.000 (57.433)
Epoch: [0][12050/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.1407 (2.9546)	Prec@1 50.000 (57.439)
Epoch: [0][12100/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.1055 (2.9540)	Prec@1 53.000 (57.448)
Epoch: [0][12150/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.8493 (2.9537)	Prec@1 60.000 (57.454)
Epoch: [0][12200/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7798 (2.9532)	Prec@1 62.000 (57.461)
Epoch: [0][12250/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.1771 (2.9529)	Prec@1 59.000 (57.463)
Epoch: [0][12300/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.8096 (2.9525)	Prec@1 62.000 (57.467)
Epoch: [0][12350/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 3.0949 (2.9523)	Prec@1 52.000 (57.467)
Epoch: [0][12400/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.9894 (2.9519)	Prec@1 55.000 (57.471)
Epoch: [0][12450/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7512 (2.9515)	Prec@1 65.000 (57.478)
Epoch: [0][12500/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.7197 (2.9511)	Prec@1 57.000 (57.486)
Epoch: [0][12550/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 2.9004 (2.9506)	Prec@1 57.000 (57.494)
Epoch: [0][12600/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.7319 (2.9501)	Prec@1 61.000 (57.501)
Epoch: [0][12650/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0377 (2.9498)	Prec@1 53.000 (57.503)
Epoch: [0][12700/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.7959 (2.9494)	Prec@1 57.000 (57.506)
Epoch: [0][12750/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 3.2121 (2.9491)	Prec@1 48.000 (57.510)
Epoch: [0][12800/12812]	Time 0.203 (0.204)	Data 0.001 (0.001)	Loss 3.0295 (2.9487)	Prec@1 53.000 (57.513)
Test: [0/500]	Time 0.056 (0.056)	Loss 0.8874 (0.8874)	Prec@1 90.000 (90.000)
Test: [50/500]	Time 0.055 (0.056)	Loss 1.0420 (1.3207)	Prec@1 82.000 (76.667)
Test: [100/500]	Time 0.055 (0.056)	Loss 1.9164 (1.4128)	Prec@1 46.000 (73.198)
Test: [150/500]	Time 0.055 (0.056)	Loss 0.9737 (1.4214)	Prec@1 88.000 (72.530)
Test: [200/500]	Time 0.055 (0.056)	Loss 2.0852 (1.4244)	Prec@1 64.000 (72.726)
Test: [250/500]	Time 0.055 (0.056)	Loss 2.2640 (1.5519)	Prec@1 54.000 (69.896)
Test: [300/500]	Time 0.055 (0.056)	Loss 3.2099 (1.6238)	Prec@1 38.000 (68.402)
Test: [350/500]	Time 0.055 (0.056)	Loss 1.6393 (1.6811)	Prec@1 68.000 (67.000)
Test: [400/500]	Time 0.055 (0.056)	Loss 1.0685 (1.7349)	Prec@1 77.000 (65.920)
Test: [450/500]	Time 0.056 (0.056)	Loss 1.5527 (1.7818)	Prec@1 73.000 (64.933)
 * Prec@1 65.264
current lr 1.00000e-04
Epoch: [1][0/12812]	Time 0.202 (0.202)	Data 0.001 (0.001)	Loss 2.6655 (2.6655)	Prec@1 67.000 (67.000)
Epoch: [1][50/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7528 (2.8511)	Prec@1 64.000 (59.078)
Epoch: [1][100/12812]	Time 0.202 (0.203)	Data 0.001 (0.001)	Loss 2.9695 (2.8593)	Prec@1 62.000 (59.178)
Epoch: [1][150/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.7945 (2.8437)	Prec@1 53.000 (59.192)
Epoch: [1][200/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 3.0309 (2.8522)	Prec@1 55.000 (59.030)
Epoch: [1][250/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.6650 (2.8482)	Prec@1 65.000 (59.088)
Epoch: [1][300/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.9378 (2.8482)	Prec@1 54.000 (59.000)
Epoch: [1][350/12812]	Time 0.202 (0.203)	Data 0.001 (0.001)	Loss 3.0398 (2.8493)	Prec@1 51.000 (58.909)
Epoch: [1][400/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 3.1068 (2.8456)	Prec@1 46.000 (58.963)
Epoch: [1][450/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7081 (2.8462)	Prec@1 60.000 (58.996)
Epoch: [1][500/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 3.1301 (2.8481)	Prec@1 49.000 (58.970)
Epoch: [1][550/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8846 (2.8456)	Prec@1 58.000 (58.929)
Epoch: [1][600/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.9688 (2.8483)	Prec@1 52.000 (58.907)
Epoch: [1][650/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.9299 (2.8471)	Prec@1 59.000 (58.985)
Epoch: [1][700/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8767 (2.8465)	Prec@1 62.000 (58.979)
Epoch: [1][750/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7858 (2.8461)	Prec@1 61.000 (59.028)
Epoch: [1][800/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.4301 (2.8449)	Prec@1 68.000 (59.061)
Epoch: [1][850/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.9436 (2.8434)	Prec@1 58.000 (59.113)
Epoch: [1][900/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.6901 (2.8441)	Prec@1 62.000 (59.122)
Epoch: [1][950/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.6547 (2.8447)	Prec@1 65.000 (59.098)
Epoch: [1][1000/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8740 (2.8449)	Prec@1 64.000 (59.118)
Epoch: [1][1050/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8222 (2.8456)	Prec@1 62.000 (59.091)
Epoch: [1][1100/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8311 (2.8462)	Prec@1 64.000 (59.093)
Epoch: [1][1150/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.6310 (2.8458)	Prec@1 65.000 (59.089)
Epoch: [1][1200/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8862 (2.8457)	Prec@1 60.000 (59.093)
Epoch: [1][1250/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.5895 (2.8455)	Prec@1 63.000 (59.067)
Epoch: [1][1300/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.6610 (2.8449)	Prec@1 66.000 (59.062)
Epoch: [1][1350/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7661 (2.8451)	Prec@1 66.000 (59.067)
Epoch: [1][1400/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7880 (2.8455)	Prec@1 57.000 (59.036)
Epoch: [1][1450/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8435 (2.8473)	Prec@1 59.000 (58.975)
Epoch: [1][1500/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.9888 (2.8467)	Prec@1 52.000 (58.983)
Epoch: [1][1550/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7660 (2.8465)	Prec@1 65.000 (59.006)
Epoch: [1][1600/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.9393 (2.8467)	Prec@1 52.000 (59.008)
Epoch: [1][1650/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.6474 (2.8453)	Prec@1 57.000 (59.032)
Epoch: [1][1700/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8512 (2.8450)	Prec@1 56.000 (59.045)
Epoch: [1][1750/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.7186 (2.8448)	Prec@1 67.000 (59.050)
Epoch: [1][1800/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.6642 (2.8446)	Prec@1 64.000 (59.035)
Epoch: [1][1850/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8575 (2.8446)	Prec@1 59.000 (59.025)
Epoch: [1][1900/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.6674 (2.8442)	Prec@1 63.000 (59.039)
Epoch: [1][1950/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8213 (2.8436)	Prec@1 59.000 (59.058)
Epoch: [1][2000/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.6340 (2.8439)	Prec@1 67.000 (59.041)
Epoch: [1][2050/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.9040 (2.8436)	Prec@1 54.000 (59.042)
Epoch: [1][2100/12812]	Time 0.206 (0.203)	Data 0.001 (0.001)	Loss 2.8985 (2.8435)	Prec@1 62.000 (59.045)
Epoch: [1][2150/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.5346 (2.8424)	Prec@1 73.000 (59.077)
Epoch: [1][2200/12812]	Time 0.202 (0.203)	Data 0.001 (0.001)	Loss 2.6554 (2.8424)	Prec@1 63.000 (59.091)
Epoch: [1][2250/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8864 (2.8409)	Prec@1 66.000 (59.131)
Epoch: [1][2300/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7003 (2.8405)	Prec@1 66.000 (59.135)
Epoch: [1][2350/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8948 (2.8403)	Prec@1 62.000 (59.135)
Epoch: [1][2400/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.9597 (2.8401)	Prec@1 58.000 (59.135)
Epoch: [1][2450/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 3.0542 (2.8401)	Prec@1 58.000 (59.143)
Epoch: [1][2500/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8833 (2.8403)	Prec@1 62.000 (59.126)
Epoch: [1][2550/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.9015 (2.8405)	Prec@1 59.000 (59.123)
Epoch: [1][2600/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.4696 (2.8403)	Prec@1 76.000 (59.131)
Epoch: [1][2650/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8973 (2.8403)	Prec@1 57.000 (59.133)
Epoch: [1][2700/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7138 (2.8404)	Prec@1 65.000 (59.138)
Epoch: [1][2750/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7649 (2.8401)	Prec@1 62.000 (59.158)
Epoch: [1][2800/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7501 (2.8403)	Prec@1 61.000 (59.148)
Epoch: [1][2850/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8381 (2.8408)	Prec@1 62.000 (59.143)
Epoch: [1][2900/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.7959 (2.8408)	Prec@1 58.000 (59.159)
Epoch: [1][2950/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 3.1324 (2.8406)	Prec@1 56.000 (59.160)
Epoch: [1][3000/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.6512 (2.8406)	Prec@1 66.000 (59.160)
Epoch: [1][3050/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.9682 (2.8399)	Prec@1 59.000 (59.184)
Epoch: [1][3100/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8962 (2.8403)	Prec@1 55.000 (59.173)
Epoch: [1][3150/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8568 (2.8401)	Prec@1 61.000 (59.185)
Epoch: [1][3200/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.9008 (2.8398)	Prec@1 56.000 (59.185)
Epoch: [1][3250/12812]	Time 0.209 (0.203)	Data 0.001 (0.001)	Loss 2.8908 (2.8403)	Prec@1 57.000 (59.173)
Epoch: [1][3300/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.7686 (2.8400)	Prec@1 58.000 (59.184)
Epoch: [1][3350/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 3.0055 (2.8399)	Prec@1 56.000 (59.172)
Epoch: [1][3400/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.9147 (2.8399)	Prec@1 58.000 (59.167)
Epoch: [1][3450/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.9897 (2.8399)	Prec@1 57.000 (59.160)
Epoch: [1][3500/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.7597 (2.8405)	Prec@1 59.000 (59.136)
Epoch: [1][3550/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8394 (2.8405)	Prec@1 55.000 (59.133)
Epoch: [1][3600/12812]	Time 0.206 (0.203)	Data 0.001 (0.001)	Loss 2.8153 (2.8400)	Prec@1 57.000 (59.153)
Epoch: [1][3650/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8626 (2.8398)	Prec@1 60.000 (59.157)
Epoch: [1][3700/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8271 (2.8394)	Prec@1 60.000 (59.166)
Epoch: [1][3750/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8158 (2.8391)	Prec@1 56.000 (59.169)
Epoch: [1][3800/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8438 (2.8394)	Prec@1 63.000 (59.153)
Epoch: [1][3850/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8447 (2.8395)	Prec@1 58.000 (59.152)
Epoch: [1][3900/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8622 (2.8396)	Prec@1 61.000 (59.139)
Epoch: [1][3950/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.6523 (2.8394)	Prec@1 63.000 (59.151)
Epoch: [1][4000/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8038 (2.8390)	Prec@1 58.000 (59.158)
Epoch: [1][4050/12812]	Time 0.205 (0.203)	Data 0.001 (0.001)	Loss 2.7747 (2.8388)	Prec@1 58.000 (59.159)
Epoch: [1][4100/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.9412 (2.8392)	Prec@1 53.000 (59.145)
Epoch: [1][4150/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8359 (2.8391)	Prec@1 56.000 (59.142)
Epoch: [1][4200/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.7695 (2.8390)	Prec@1 64.000 (59.139)
Epoch: [1][4250/12812]	Time 0.205 (0.203)	Data 0.001 (0.001)	Loss 2.7158 (2.8393)	Prec@1 59.000 (59.141)
Epoch: [1][4300/12812]	Time 0.205 (0.203)	Data 0.001 (0.001)	Loss 2.8638 (2.8392)	Prec@1 53.000 (59.139)
Epoch: [1][4350/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.7699 (2.8388)	Prec@1 69.000 (59.163)
Epoch: [1][4400/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 3.0630 (2.8387)	Prec@1 53.000 (59.163)
Epoch: [1][4450/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8071 (2.8382)	Prec@1 60.000 (59.168)
Epoch: [1][4500/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8281 (2.8380)	Prec@1 59.000 (59.174)
Epoch: [1][4550/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8965 (2.8382)	Prec@1 65.000 (59.169)
Epoch: [1][4600/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.8596 (2.8378)	Prec@1 66.000 (59.183)
Epoch: [1][4650/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.6996 (2.8381)	Prec@1 68.000 (59.178)
Epoch: [1][4700/12812]	Time 0.204 (0.203)	Data 0.001 (0.001)	Loss 2.7001 (2.8383)	Prec@1 65.000 (59.179)
Epoch: [1][4750/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 3.0037 (2.8381)	Prec@1 55.000 (59.179)
Epoch: [1][4800/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.9551 (2.8377)	Prec@1 58.000 (59.189)
Epoch: [1][4850/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.7072 (2.8374)	Prec@1 58.000 (59.198)
Epoch: [1][4900/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.7132 (2.8372)	Prec@1 63.000 (59.204)
Epoch: [1][4950/12812]	Time 0.206 (0.204)	Data 0.001 (0.001)	Loss 2.8952 (2.8371)	Prec@1 64.000 (59.208)
Epoch: [1][5000/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.7823 (2.8371)	Prec@1 58.000 (59.205)
Epoch: [1][5050/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 3.0094 (2.8372)	Prec@1 54.000 (59.192)
Epoch: [1][5100/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.7357 (2.8374)	Prec@1 61.000 (59.184)
Epoch: [1][5150/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.8381 (2.8374)	Prec@1 60.000 (59.177)
Epoch: [1][5200/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.9189 (2.8372)	Prec@1 58.000 (59.187)
Epoch: [1][5250/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.7464 (2.8371)	Prec@1 63.000 (59.186)
Epoch: [1][5300/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.8128 (2.8370)	Prec@1 60.000 (59.189)
Epoch: [1][5350/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.4095 (2.8369)	Prec@1 71.000 (59.195)
Epoch: [1][5400/12812]	Time 0.206 (0.204)	Data 0.001 (0.001)	Loss 2.8341 (2.8371)	Prec@1 56.000 (59.196)
Epoch: [1][5450/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.6176 (2.8368)	Prec@1 64.000 (59.201)
Epoch: [1][5500/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.6883 (2.8365)	Prec@1 63.000 (59.206)
Epoch: [1][5550/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.6849 (2.8360)	Prec@1 64.000 (59.224)
Epoch: [1][5600/12812]	Time 0.206 (0.204)	Data 0.001 (0.001)	Loss 3.1556 (2.8359)	Prec@1 52.000 (59.228)
Epoch: [1][5650/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.7712 (2.8359)	Prec@1 65.000 (59.224)
Epoch: [1][5700/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.7868 (2.8360)	Prec@1 60.000 (59.223)
Epoch: [1][5750/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.6611 (2.8355)	Prec@1 62.000 (59.235)
Epoch: [1][5800/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.9285 (2.8358)	Prec@1 54.000 (59.235)
Epoch: [1][5850/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.8997 (2.8358)	Prec@1 60.000 (59.231)
Epoch: [1][5900/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.5297 (2.8356)	Prec@1 61.000 (59.236)
Epoch: [1][5950/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.8058 (2.8355)	Prec@1 62.000 (59.239)
Epoch: [1][6000/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.8964 (2.8355)	Prec@1 60.000 (59.243)
Epoch: [1][6050/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 3.0834 (2.8354)	Prec@1 55.000 (59.241)
Epoch: [1][6100/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.9300 (2.8353)	Prec@1 59.000 (59.245)
Epoch: [1][6150/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.7355 (2.8351)	Prec@1 64.000 (59.254)
Epoch: [1][6200/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.6481 (2.8350)	Prec@1 63.000 (59.253)
Epoch: [1][6250/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.8117 (2.8349)	Prec@1 61.000 (59.257)
Epoch: [1][6300/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 3.0568 (2.8350)	Prec@1 55.000 (59.261)
Epoch: [1][6350/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.9243 (2.8350)	Prec@1 58.000 (59.269)
Epoch: [1][6400/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.9021 (2.8352)	Prec@1 56.000 (59.268)
Epoch: [1][6450/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.7024 (2.8352)	Prec@1 64.000 (59.274)
Epoch: [1][6500/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.8201 (2.8351)	Prec@1 50.000 (59.274)
Epoch: [1][6550/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.8260 (2.8349)	Prec@1 58.000 (59.285)
Epoch: [1][6600/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.7736 (2.8350)	Prec@1 62.000 (59.281)
Epoch: [1][6650/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.6370 (2.8353)	Prec@1 62.000 (59.276)
Epoch: [1][6700/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.7833 (2.8349)	Prec@1 61.000 (59.284)
Epoch: [1][6750/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.7812 (2.8347)	Prec@1 60.000 (59.286)
Epoch: [1][6800/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.9359 (2.8346)	Prec@1 52.000 (59.286)
Epoch: [1][6850/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 2.8442 (2.8344)	Prec@1 56.000 (59.291)
Epoch: [1][6900/12812]	Time 0.205 (0.204)	Data 0.001 (0.001)	Loss 3.0393 (2.8343)	Prec@1 51.000 (59.294)
Epoch: [1][6950/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.9955 (2.8342)	Prec@1 52.000 (59.295)
Epoch: [1][7000/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 3.0351 (2.8342)	Prec@1 54.000 (59.298)
Epoch: [1][7050/12812]	Time 0.206 (0.204)	Data 0.001 (0.001)	Loss 2.8395 (2.8342)	Prec@1 58.000 (59.297)
Epoch: [1][7100/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.8453 (2.8341)	Prec@1 65.000 (59.308)
Epoch: [1][7150/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.7362 (2.8339)	Prec@1 63.000 (59.316)
Epoch: [1][7200/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.8930 (2.8338)	Prec@1 62.000 (59.321)
Epoch: [1][7250/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.8155 (2.8336)	Prec@1 56.000 (59.323)
Epoch: [1][7300/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.9902 (2.8334)	Prec@1 55.000 (59.328)
Epoch: [1][7350/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.5681 (2.8335)	Prec@1 58.000 (59.324)
Epoch: [1][7400/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.5206 (2.8333)	Prec@1 68.000 (59.328)
Epoch: [1][7450/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.7291 (2.8333)	Prec@1 59.000 (59.330)
Epoch: [1][7500/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.6909 (2.8332)	Prec@1 59.000 (59.331)
Epoch: [1][7550/12812]	Time 0.208 (0.204)	Data 0.001 (0.001)	Loss 2.9805 (2.8334)	Prec@1 55.000 (59.329)
Epoch: [1][7600/12812]	Time 0.209 (0.204)	Data 0.001 (0.001)	Loss 2.7099 (2.8336)	Prec@1 67.000 (59.324)
Epoch: [1][7650/12812]	Time 0.208 (0.204)	Data 0.001 (0.001)	Loss 2.9112 (2.8336)	Prec@1 60.000 (59.330)
Epoch: [1][7700/12812]	Time 0.208 (0.204)	Data 0.001 (0.001)	Loss 2.7007 (2.8335)	Prec@1 59.000 (59.329)
Epoch: [1][7750/12812]	Time 0.210 (0.204)	Data 0.001 (0.001)	Loss 2.9552 (2.8337)	Prec@1 58.000 (59.318)
Epoch: [1][7800/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.9834 (2.8334)	Prec@1 57.000 (59.321)
Epoch: [1][7850/12812]	Time 0.209 (0.204)	Data 0.001 (0.001)	Loss 2.7171 (2.8332)	Prec@1 62.000 (59.327)
Epoch: [1][7900/12812]	Time 0.209 (0.204)	Data 0.001 (0.001)	Loss 3.0550 (2.8333)	Prec@1 54.000 (59.329)
Epoch: [1][7950/12812]	Time 0.206 (0.204)	Data 0.001 (0.001)	Loss 2.9260 (2.8331)	Prec@1 58.000 (59.331)
Epoch: [1][8000/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.8522 (2.8330)	Prec@1 56.000 (59.330)
Epoch: [1][8050/12812]	Time 0.207 (0.204)	Data 0.001 (0.001)	Loss 2.6372 (2.8328)	Prec@1 70.000 (59.335)
Epoch: [1][8100/12812]	Time 0.208 (0.204)	Data 0.001 (0.001)	Loss 2.7778 (2.8328)	Prec@1 61.000 (59.337)
Epoch: [1][8150/12812]	Time 0.208 (0.204)	Data 0.001 (0.001)	Loss 2.8656 (2.8329)	Prec@1 62.000 (59.336)
Epoch: [1][8200/12812]	Time 0.209 (0.204)	Data 0.001 (0.001)	Loss 2.7168 (2.8328)	Prec@1 68.000 (59.341)
Epoch: [1][8250/12812]	Time 0.208 (0.204)	Data 0.001 (0.001)	Loss 2.9016 (2.8326)	Prec@1 60.000 (59.346)
Epoch: [1][8300/12812]	Time 0.208 (0.204)	Data 0.001 (0.001)	Loss 2.8285 (2.8325)	Prec@1 63.000 (59.345)
Epoch: [1][8350/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.8134 (2.8323)	Prec@1 58.000 (59.349)
Epoch: [1][8400/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 3.1058 (2.8321)	Prec@1 54.000 (59.354)
Epoch: [1][8450/12812]	Time 0.210 (0.205)	Data 0.001 (0.001)	Loss 3.0750 (2.8321)	Prec@1 51.000 (59.358)
Epoch: [1][8500/12812]	Time 0.207 (0.205)	Data 0.000 (0.001)	Loss 2.9333 (2.8321)	Prec@1 64.000 (59.359)
Epoch: [1][8550/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.8680 (2.8321)	Prec@1 63.000 (59.354)
Epoch: [1][8600/12812]	Time 0.209 (0.205)	Data 0.001 (0.001)	Loss 2.5679 (2.8319)	Prec@1 68.000 (59.360)
Epoch: [1][8650/12812]	Time 0.210 (0.205)	Data 0.001 (0.001)	Loss 2.9065 (2.8318)	Prec@1 53.000 (59.359)
Epoch: [1][8700/12812]	Time 0.208 (0.205)	Data 0.000 (0.001)	Loss 2.7870 (2.8317)	Prec@1 61.000 (59.363)
Epoch: [1][8750/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.7998 (2.8316)	Prec@1 59.000 (59.367)
Epoch: [1][8800/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.9481 (2.8315)	Prec@1 58.000 (59.371)
Epoch: [1][8850/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.7469 (2.8312)	Prec@1 65.000 (59.377)
Epoch: [1][8900/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.5112 (2.8313)	Prec@1 60.000 (59.375)
Epoch: [1][8950/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 3.0939 (2.8312)	Prec@1 54.000 (59.381)
Epoch: [1][9000/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.7900 (2.8309)	Prec@1 58.000 (59.387)
Epoch: [1][9050/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.6700 (2.8310)	Prec@1 68.000 (59.388)
Epoch: [1][9100/12812]	Time 0.206 (0.205)	Data 0.001 (0.001)	Loss 2.5991 (2.8308)	Prec@1 66.000 (59.394)
Epoch: [1][9150/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 3.0695 (2.8307)	Prec@1 56.000 (59.393)
Epoch: [1][9200/12812]	Time 0.206 (0.205)	Data 0.001 (0.001)	Loss 3.0432 (2.8307)	Prec@1 57.000 (59.391)
Epoch: [1][9250/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.9632 (2.8306)	Prec@1 56.000 (59.396)
Epoch: [1][9300/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.5143 (2.8306)	Prec@1 69.000 (59.392)
Epoch: [1][9350/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.9641 (2.8305)	Prec@1 58.000 (59.397)
Epoch: [1][9400/12812]	Time 0.206 (0.205)	Data 0.001 (0.001)	Loss 2.7205 (2.8305)	Prec@1 66.000 (59.396)
Epoch: [1][9450/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.8748 (2.8304)	Prec@1 61.000 (59.399)
Epoch: [1][9500/12812]	Time 0.209 (0.205)	Data 0.001 (0.001)	Loss 2.8867 (2.8304)	Prec@1 61.000 (59.400)
Epoch: [1][9550/12812]	Time 0.209 (0.205)	Data 0.001 (0.001)	Loss 2.9108 (2.8303)	Prec@1 62.000 (59.402)
Epoch: [1][9600/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.8080 (2.8301)	Prec@1 61.000 (59.406)
Epoch: [1][9650/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 3.0192 (2.8301)	Prec@1 53.000 (59.403)
Epoch: [1][9700/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.7414 (2.8300)	Prec@1 63.000 (59.404)
Epoch: [1][9750/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.7011 (2.8299)	Prec@1 64.000 (59.405)
Epoch: [1][9800/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.5900 (2.8299)	Prec@1 61.000 (59.402)
Epoch: [1][9850/12812]	Time 0.206 (0.205)	Data 0.000 (0.001)	Loss 3.0654 (2.8300)	Prec@1 50.000 (59.400)
Epoch: [1][9900/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 3.2114 (2.8300)	Prec@1 46.000 (59.397)
Epoch: [1][9950/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.8238 (2.8301)	Prec@1 59.000 (59.399)
Epoch: [1][10000/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.6488 (2.8303)	Prec@1 61.000 (59.395)
Epoch: [1][10050/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.7328 (2.8303)	Prec@1 60.000 (59.395)
Epoch: [1][10100/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.5594 (2.8301)	Prec@1 66.000 (59.399)
Epoch: [1][10150/12812]	Time 0.206 (0.205)	Data 0.001 (0.001)	Loss 2.9268 (2.8300)	Prec@1 59.000 (59.406)
Epoch: [1][10200/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.5783 (2.8301)	Prec@1 64.000 (59.401)
Epoch: [1][10250/12812]	Time 0.209 (0.205)	Data 0.001 (0.001)	Loss 2.6152 (2.8300)	Prec@1 60.000 (59.405)
Epoch: [1][10300/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.5999 (2.8299)	Prec@1 63.000 (59.413)
Epoch: [1][10350/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.7457 (2.8298)	Prec@1 68.000 (59.415)
Epoch: [1][10400/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.9903 (2.8295)	Prec@1 54.000 (59.424)
Epoch: [1][10450/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 3.0073 (2.8294)	Prec@1 58.000 (59.423)
Epoch: [1][10500/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.7625 (2.8293)	Prec@1 59.000 (59.426)
Epoch: [1][10550/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.8793 (2.8292)	Prec@1 64.000 (59.427)
Epoch: [1][10600/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.7290 (2.8293)	Prec@1 60.000 (59.426)
Epoch: [1][10650/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.5277 (2.8291)	Prec@1 64.000 (59.432)
Epoch: [1][10700/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.8050 (2.8290)	Prec@1 62.000 (59.433)
Epoch: [1][10750/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.9549 (2.8291)	Prec@1 58.000 (59.430)
Epoch: [1][10800/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.5057 (2.8289)	Prec@1 64.000 (59.434)
Epoch: [1][10850/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.7727 (2.8289)	Prec@1 63.000 (59.435)
Epoch: [1][10900/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.6544 (2.8288)	Prec@1 58.000 (59.436)
Epoch: [1][10950/12812]	Time 0.209 (0.205)	Data 0.001 (0.001)	Loss 2.6068 (2.8287)	Prec@1 60.000 (59.435)
Epoch: [1][11000/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.6666 (2.8287)	Prec@1 68.000 (59.433)
Epoch: [1][11050/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.9591 (2.8287)	Prec@1 59.000 (59.434)
Epoch: [1][11100/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.9829 (2.8285)	Prec@1 56.000 (59.439)
Epoch: [1][11150/12812]	Time 0.213 (0.205)	Data 0.001 (0.001)	Loss 2.9652 (2.8285)	Prec@1 59.000 (59.439)
Epoch: [1][11200/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.6055 (2.8284)	Prec@1 68.000 (59.443)
Epoch: [1][11250/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.8192 (2.8285)	Prec@1 61.000 (59.442)
Epoch: [1][11300/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 3.1208 (2.8284)	Prec@1 50.000 (59.445)
Epoch: [1][11350/12812]	Time 0.209 (0.205)	Data 0.001 (0.001)	Loss 2.9931 (2.8284)	Prec@1 60.000 (59.449)
Epoch: [1][11400/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.7028 (2.8284)	Prec@1 63.000 (59.445)
Epoch: [1][11450/12812]	Time 0.209 (0.205)	Data 0.001 (0.001)	Loss 2.5838 (2.8283)	Prec@1 68.000 (59.448)
Epoch: [1][11500/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.7796 (2.8283)	Prec@1 66.000 (59.449)
Epoch: [1][11550/12812]	Time 0.207 (0.205)	Data 0.001 (0.001)	Loss 2.8149 (2.8281)	Prec@1 57.000 (59.452)
Epoch: [1][11600/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.9344 (2.8282)	Prec@1 55.000 (59.448)
Epoch: [1][11650/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.9487 (2.8281)	Prec@1 60.000 (59.449)
Epoch: [1][11700/12812]	Time 0.208 (0.205)	Data 0.001 (0.001)	Loss 2.9879 (2.8280)	Prec@1 63.000 (59.449)
Epoch: [1][11750/12812]	Time 0.211 (0.205)	Data 0.001 (0.001)	Loss 2.9611 (2.8280)	Prec@1 56.000 (59.450)
Epoch: [1][11800/12812]	Time 0.208 (0.206)	Data 0.001 (0.001)	Loss 2.7545 (2.8280)	Prec@1 62.000 (59.452)
Epoch: [1][11850/12812]	Time 0.206 (0.206)	Data 0.001 (0.001)	Loss 2.9434 (2.8280)	Prec@1 56.000 (59.456)
Epoch: [1][11900/12812]	Time 0.207 (0.206)	Data 0.001 (0.001)	Loss 2.8594 (2.8279)	Prec@1 59.000 (59.459)
Epoch: [1][11950/12812]	Time 0.207 (0.206)	Data 0.001 (0.001)	Loss 2.8510 (2.8278)	Prec@1 55.000 (59.463)
Epoch: [1][12000/12812]	Time 0.206 (0.206)	Data 0.001 (0.001)	Loss 2.6625 (2.8279)	Prec@1 67.000 (59.463)
Epoch: [1][12050/12812]	Time 0.206 (0.206)	Data 0.001 (0.001)	Loss 2.7525 (2.8278)	Prec@1 57.000 (59.465)
Epoch: [1][12100/12812]	Time 0.205 (0.206)	Data 0.001 (0.001)	Loss 2.8513 (2.8277)	Prec@1 55.000 (59.469)
Epoch: [1][12150/12812]	Time 0.205 (0.206)	Data 0.001 (0.001)	Loss 2.6565 (2.8277)	Prec@1 63.000 (59.468)
Epoch: [1][12200/12812]	Time 0.205 (0.206)	Data 0.001 (0.001)	Loss 2.7425 (2.8277)	Prec@1 61.000 (59.465)
Epoch: [1][12250/12812]	Time 0.206 (0.206)	Data 0.001 (0.001)	Loss 3.0464 (2.8279)	Prec@1 50.000 (59.459)
Epoch: [1][12300/12812]	Time 0.205 (0.206)	Data 0.001 (0.001)	Loss 2.7713 (2.8279)	Prec@1 64.000 (59.455)
Epoch: [1][12350/12812]	Time 0.206 (0.206)	Data 0.001 (0.001)	Loss 2.7141 (2.8280)	Prec@1 61.000 (59.450)
Epoch: [1][12400/12812]	Time 0.205 (0.206)	Data 0.001 (0.001)	Loss 2.6326 (2.8281)	Prec@1 66.000 (59.452)
Epoch: [1][12450/12812]	Time 0.206 (0.206)	Data 0.001 (0.001)	Loss 2.8367 (2.8280)	Prec@1 58.000 (59.458)
Epoch: [1][12500/12812]	Time 0.206 (0.206)	Data 0.001 (0.001)	Loss 2.9690 (2.8281)	Prec@1 56.000 (59.459)
Epoch: [1][12550/12812]	Time 0.206 (0.206)	Data 0.001 (0.001)	Loss 2.6560 (2.8280)	Prec@1 66.000 (59.462)
Epoch: [1][12600/12812]	Time 0.206 (0.206)	Data 0.001 (0.001)	Loss 2.8646 (2.8279)	Prec@1 54.000 (59.464)
Epoch: [1][12650/12812]	Time 0.207 (0.206)	Data 0.001 (0.001)	Loss 2.8626 (2.8280)	Prec@1 58.000 (59.461)
Epoch: [1][12700/12812]	Time 0.207 (0.206)	Data 0.001 (0.001)	Loss 2.8645 (2.8281)	Prec@1 58.000 (59.457)
Epoch: [1][12750/12812]	Time 0.209 (0.206)	Data 0.001 (0.001)	Loss 2.8424 (2.8281)	Prec@1 57.000 (59.458)
Epoch: [1][12800/12812]	Time 0.208 (0.206)	Data 0.001 (0.001)	Loss 2.9071 (2.8281)	Prec@1 58.000 (59.457)
Test: [0/500]	Time 0.058 (0.058)	Loss 0.8829 (0.8829)	Prec@1 90.000 (90.000)
Test: [50/500]	Time 0.058 (0.057)	Loss 1.0308 (1.3006)	Prec@1 80.000 (76.549)
Test: [100/500]	Time 0.058 (0.057)	Loss 1.8934 (1.3874)	Prec@1 49.000 (73.604)
Test: [150/500]	Time 0.057 (0.057)	Loss 0.8897 (1.3896)	Prec@1 89.000 (73.073)
Test: [200/500]	Time 0.058 (0.057)	Loss 2.0124 (1.3898)	Prec@1 62.000 (73.294)
Test: [250/500]	Time 0.057 (0.057)	Loss 2.2696 (1.5209)	Prec@1 55.000 (70.422)
Test: [300/500]	Time 0.056 (0.057)	Loss 3.2214 (1.5940)	Prec@1 39.000 (68.963)
Test: [350/500]	Time 0.057 (0.057)	Loss 1.6423 (1.6542)	Prec@1 67.000 (67.553)
Test: [400/500]	Time 0.056 (0.057)	Loss 1.0450 (1.7081)	Prec@1 80.000 (66.504)
Test: [450/500]	Time 0.057 (0.057)	Loss 1.5320 (1.7556)	Prec@1 73.000 (65.466)
 * Prec@1 65.758
current lr 1.00000e-04
Epoch: [2][0/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.8677 (2.8677)	Prec@1 58.000 (58.000)
Epoch: [2][50/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.6285 (2.7942)	Prec@1 71.000 (60.039)
Epoch: [2][100/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 2.6772 (2.8276)	Prec@1 59.000 (59.495)
Epoch: [2][150/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7379 (2.8106)	Prec@1 61.000 (59.927)
Epoch: [2][200/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7839 (2.8114)	Prec@1 61.000 (59.915)
Epoch: [2][250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0288 (2.8097)	Prec@1 50.000 (59.932)
Epoch: [2][300/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8120 (2.8046)	Prec@1 61.000 (60.076)
Epoch: [2][350/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.8108 (2.8070)	Prec@1 59.000 (60.017)
Epoch: [2][400/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8261 (2.8052)	Prec@1 57.000 (60.007)
Epoch: [2][450/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8121 (2.8100)	Prec@1 59.000 (59.929)
Epoch: [2][500/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9963 (2.8106)	Prec@1 56.000 (59.952)
Epoch: [2][550/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8423 (2.8150)	Prec@1 57.000 (59.797)
Epoch: [2][600/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7680 (2.8189)	Prec@1 57.000 (59.657)
Epoch: [2][650/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0146 (2.8198)	Prec@1 58.000 (59.639)
Epoch: [2][700/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8328 (2.8198)	Prec@1 59.000 (59.579)
Epoch: [2][750/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7652 (2.8206)	Prec@1 63.000 (59.569)
Epoch: [2][800/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7203 (2.8192)	Prec@1 57.000 (59.613)
Epoch: [2][850/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9395 (2.8175)	Prec@1 57.000 (59.662)
Epoch: [2][900/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7559 (2.8180)	Prec@1 57.000 (59.644)
Epoch: [2][950/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9411 (2.8189)	Prec@1 54.000 (59.615)
Epoch: [2][1000/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.2547 (2.8198)	Prec@1 43.000 (59.595)
Epoch: [2][1050/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7780 (2.8213)	Prec@1 62.000 (59.515)
Epoch: [2][1100/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8832 (2.8219)	Prec@1 59.000 (59.499)
Epoch: [2][1150/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6449 (2.8216)	Prec@1 67.000 (59.566)
Epoch: [2][1200/12812]	Time 0.205 (0.208)	Data 0.001 (0.001)	Loss 2.7512 (2.8220)	Prec@1 63.000 (59.559)
Epoch: [2][1250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8461 (2.8226)	Prec@1 62.000 (59.528)
Epoch: [2][1300/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8926 (2.8217)	Prec@1 55.000 (59.540)
Epoch: [2][1350/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8758 (2.8229)	Prec@1 62.000 (59.523)
Epoch: [2][1400/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9905 (2.8232)	Prec@1 58.000 (59.495)
Epoch: [2][1450/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8262 (2.8234)	Prec@1 64.000 (59.474)
Epoch: [2][1500/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9535 (2.8234)	Prec@1 59.000 (59.491)
Epoch: [2][1550/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0325 (2.8232)	Prec@1 58.000 (59.501)
Epoch: [2][1600/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.6351 (2.8222)	Prec@1 65.000 (59.536)
Epoch: [2][1650/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9725 (2.8212)	Prec@1 54.000 (59.569)
Epoch: [2][1700/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9238 (2.8207)	Prec@1 57.000 (59.572)
Epoch: [2][1750/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9293 (2.8214)	Prec@1 60.000 (59.549)
Epoch: [2][1800/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6022 (2.8211)	Prec@1 64.000 (59.579)
Epoch: [2][1850/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8415 (2.8208)	Prec@1 60.000 (59.578)
Epoch: [2][1900/12812]	Time 0.211 (0.208)	Data 0.001 (0.001)	Loss 2.8007 (2.8214)	Prec@1 62.000 (59.564)
Epoch: [2][1950/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.8141 (2.8207)	Prec@1 54.000 (59.598)
Epoch: [2][2000/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9258 (2.8210)	Prec@1 56.000 (59.594)
Epoch: [2][2050/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.8985 (2.8208)	Prec@1 58.000 (59.586)
Epoch: [2][2100/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.7581 (2.8205)	Prec@1 63.000 (59.607)
Epoch: [2][2150/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.8576 (2.8199)	Prec@1 62.000 (59.643)
Epoch: [2][2200/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8652 (2.8201)	Prec@1 54.000 (59.643)
Epoch: [2][2250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0219 (2.8202)	Prec@1 56.000 (59.672)
Epoch: [2][2300/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9205 (2.8200)	Prec@1 61.000 (59.689)
Epoch: [2][2350/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 3.0590 (2.8200)	Prec@1 53.000 (59.677)
Epoch: [2][2400/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9804 (2.8198)	Prec@1 55.000 (59.676)
Epoch: [2][2450/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9244 (2.8201)	Prec@1 57.000 (59.663)
Epoch: [2][2500/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9079 (2.8198)	Prec@1 57.000 (59.673)
Epoch: [2][2550/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0024 (2.8201)	Prec@1 53.000 (59.654)
Epoch: [2][2600/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7820 (2.8202)	Prec@1 61.000 (59.657)
Epoch: [2][2650/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8507 (2.8197)	Prec@1 65.000 (59.680)
Epoch: [2][2700/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8640 (2.8199)	Prec@1 61.000 (59.687)
Epoch: [2][2750/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9066 (2.8193)	Prec@1 48.000 (59.700)
Epoch: [2][2800/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8185 (2.8204)	Prec@1 57.000 (59.682)
Epoch: [2][2850/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6517 (2.8208)	Prec@1 67.000 (59.669)
Epoch: [2][2900/12812]	Time 0.205 (0.208)	Data 0.001 (0.001)	Loss 2.7429 (2.8213)	Prec@1 58.000 (59.658)
Epoch: [2][2950/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.1336 (2.8209)	Prec@1 53.000 (59.667)
Epoch: [2][3000/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9889 (2.8205)	Prec@1 56.000 (59.676)
Epoch: [2][3050/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.1445 (2.8203)	Prec@1 53.000 (59.689)
Epoch: [2][3100/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.8493 (2.8211)	Prec@1 61.000 (59.684)
Epoch: [2][3150/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.5473 (2.8211)	Prec@1 72.000 (59.691)
Epoch: [2][3200/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7416 (2.8210)	Prec@1 60.000 (59.696)
Epoch: [2][3250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6760 (2.8214)	Prec@1 66.000 (59.696)
Epoch: [2][3300/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9496 (2.8212)	Prec@1 54.000 (59.704)
Epoch: [2][3350/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9727 (2.8211)	Prec@1 62.000 (59.704)
Epoch: [2][3400/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 2.7235 (2.8212)	Prec@1 61.000 (59.708)
Epoch: [2][3450/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 3.0571 (2.8216)	Prec@1 57.000 (59.693)
Epoch: [2][3500/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9568 (2.8217)	Prec@1 60.000 (59.685)
Epoch: [2][3550/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6732 (2.8218)	Prec@1 65.000 (59.688)
Epoch: [2][3600/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6837 (2.8213)	Prec@1 63.000 (59.705)
Epoch: [2][3650/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.7752 (2.8215)	Prec@1 64.000 (59.701)
Epoch: [2][3700/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9294 (2.8209)	Prec@1 59.000 (59.716)
Epoch: [2][3750/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6454 (2.8208)	Prec@1 62.000 (59.719)
Epoch: [2][3800/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7409 (2.8206)	Prec@1 59.000 (59.727)
Epoch: [2][3850/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6976 (2.8207)	Prec@1 62.000 (59.720)
Epoch: [2][3900/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7571 (2.8209)	Prec@1 60.000 (59.715)
Epoch: [2][3950/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6879 (2.8210)	Prec@1 61.000 (59.709)
Epoch: [2][4000/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7421 (2.8205)	Prec@1 62.000 (59.708)
Epoch: [2][4050/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0183 (2.8205)	Prec@1 55.000 (59.705)
Epoch: [2][4100/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.8198 (2.8209)	Prec@1 61.000 (59.703)
Epoch: [2][4150/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.1428 (2.8209)	Prec@1 51.000 (59.709)
Epoch: [2][4200/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9861 (2.8216)	Prec@1 58.000 (59.686)
Epoch: [2][4250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9044 (2.8216)	Prec@1 57.000 (59.686)
Epoch: [2][4300/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9214 (2.8219)	Prec@1 59.000 (59.679)
Epoch: [2][4350/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6773 (2.8218)	Prec@1 65.000 (59.682)
Epoch: [2][4400/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8785 (2.8218)	Prec@1 58.000 (59.690)
Epoch: [2][4450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8905 (2.8216)	Prec@1 58.000 (59.687)
Epoch: [2][4500/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6439 (2.8213)	Prec@1 68.000 (59.690)
Epoch: [2][4550/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.4776 (2.8215)	Prec@1 64.000 (59.688)
Epoch: [2][4600/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8705 (2.8215)	Prec@1 64.000 (59.693)
Epoch: [2][4650/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8558 (2.8220)	Prec@1 58.000 (59.683)
Epoch: [2][4700/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7949 (2.8219)	Prec@1 62.000 (59.681)
Epoch: [2][4750/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9981 (2.8218)	Prec@1 54.000 (59.685)
Epoch: [2][4800/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0070 (2.8216)	Prec@1 53.000 (59.695)
Epoch: [2][4850/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6934 (2.8215)	Prec@1 67.000 (59.695)
Epoch: [2][4900/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0020 (2.8219)	Prec@1 55.000 (59.691)
Epoch: [2][4950/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0183 (2.8220)	Prec@1 48.000 (59.683)
Epoch: [2][5000/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7723 (2.8218)	Prec@1 62.000 (59.683)
Epoch: [2][5050/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 2.6345 (2.8221)	Prec@1 62.000 (59.677)
Epoch: [2][5100/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 3.3841 (2.8222)	Prec@1 45.000 (59.669)
Epoch: [2][5150/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7984 (2.8223)	Prec@1 61.000 (59.664)
Epoch: [2][5200/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.6345 (2.8227)	Prec@1 64.000 (59.658)
Epoch: [2][5250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7846 (2.8226)	Prec@1 57.000 (59.659)
Epoch: [2][5300/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8801 (2.8226)	Prec@1 61.000 (59.659)
Epoch: [2][5350/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9314 (2.8225)	Prec@1 64.000 (59.660)
Epoch: [2][5400/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7535 (2.8229)	Prec@1 60.000 (59.659)
Epoch: [2][5450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8135 (2.8230)	Prec@1 60.000 (59.667)
Epoch: [2][5500/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7849 (2.8225)	Prec@1 64.000 (59.682)
Epoch: [2][5550/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8437 (2.8224)	Prec@1 59.000 (59.686)
Epoch: [2][5600/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9029 (2.8222)	Prec@1 53.000 (59.693)
Epoch: [2][5650/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.5520 (2.8225)	Prec@1 60.000 (59.684)
Epoch: [2][5700/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9711 (2.8229)	Prec@1 51.000 (59.672)
Epoch: [2][5750/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7029 (2.8228)	Prec@1 65.000 (59.670)
Epoch: [2][5800/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9239 (2.8232)	Prec@1 58.000 (59.669)
Epoch: [2][5850/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 2.7649 (2.8232)	Prec@1 66.000 (59.667)
Epoch: [2][5900/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8662 (2.8229)	Prec@1 57.000 (59.670)
Epoch: [2][5950/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7085 (2.8231)	Prec@1 69.000 (59.668)
Epoch: [2][6000/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9791 (2.8233)	Prec@1 51.000 (59.667)
Epoch: [2][6050/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0190 (2.8232)	Prec@1 52.000 (59.670)
Epoch: [2][6100/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9239 (2.8234)	Prec@1 53.000 (59.670)
Epoch: [2][6150/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7006 (2.8233)	Prec@1 67.000 (59.675)
Epoch: [2][6200/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9674 (2.8233)	Prec@1 51.000 (59.673)
Epoch: [2][6250/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.4663 (2.8233)	Prec@1 71.000 (59.668)
Epoch: [2][6300/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9735 (2.8236)	Prec@1 59.000 (59.662)
Epoch: [2][6350/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 3.0252 (2.8238)	Prec@1 54.000 (59.655)
Epoch: [2][6400/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0224 (2.8239)	Prec@1 58.000 (59.653)
Epoch: [2][6450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7610 (2.8241)	Prec@1 61.000 (59.652)
Epoch: [2][6500/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7770 (2.8242)	Prec@1 63.000 (59.653)
Epoch: [2][6550/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8615 (2.8242)	Prec@1 62.000 (59.654)
Epoch: [2][6600/12812]	Time 0.211 (0.208)	Data 0.001 (0.001)	Loss 2.7779 (2.8244)	Prec@1 58.000 (59.651)
Epoch: [2][6650/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.8476 (2.8247)	Prec@1 60.000 (59.641)
Epoch: [2][6700/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.6731 (2.8247)	Prec@1 66.000 (59.637)
Epoch: [2][6750/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9015 (2.8248)	Prec@1 52.000 (59.634)
Epoch: [2][6800/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8614 (2.8246)	Prec@1 58.000 (59.637)
Epoch: [2][6850/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6362 (2.8245)	Prec@1 61.000 (59.637)
Epoch: [2][6900/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.4813 (2.8246)	Prec@1 70.000 (59.632)
Epoch: [2][6950/12812]	Time 0.207 (0.208)	Data 0.000 (0.001)	Loss 2.8253 (2.8246)	Prec@1 67.000 (59.635)
Epoch: [2][7000/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9147 (2.8246)	Prec@1 53.000 (59.633)
Epoch: [2][7050/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0148 (2.8247)	Prec@1 53.000 (59.627)
Epoch: [2][7100/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9029 (2.8248)	Prec@1 54.000 (59.628)
Epoch: [2][7150/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 3.0580 (2.8248)	Prec@1 54.000 (59.632)
Epoch: [2][7200/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.5872 (2.8246)	Prec@1 63.000 (59.639)
Epoch: [2][7250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6662 (2.8246)	Prec@1 60.000 (59.635)
Epoch: [2][7300/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6761 (2.8244)	Prec@1 60.000 (59.644)
Epoch: [2][7350/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9561 (2.8246)	Prec@1 56.000 (59.641)
Epoch: [2][7400/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8151 (2.8245)	Prec@1 62.000 (59.645)
Epoch: [2][7450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8506 (2.8244)	Prec@1 56.000 (59.648)
Epoch: [2][7500/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.5356 (2.8244)	Prec@1 69.000 (59.646)
Epoch: [2][7550/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8247 (2.8243)	Prec@1 59.000 (59.650)
Epoch: [2][7600/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0134 (2.8245)	Prec@1 53.000 (59.648)
Epoch: [2][7650/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.2233 (2.8245)	Prec@1 56.000 (59.653)
Epoch: [2][7700/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.6864 (2.8246)	Prec@1 58.000 (59.653)
Epoch: [2][7750/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8003 (2.8246)	Prec@1 65.000 (59.652)
Epoch: [2][7800/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8393 (2.8248)	Prec@1 61.000 (59.645)
Epoch: [2][7850/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8457 (2.8247)	Prec@1 59.000 (59.649)
Epoch: [2][7900/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7584 (2.8247)	Prec@1 67.000 (59.649)
Epoch: [2][7950/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8756 (2.8248)	Prec@1 57.000 (59.646)
Epoch: [2][8000/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8400 (2.8245)	Prec@1 58.000 (59.658)
Epoch: [2][8050/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6399 (2.8245)	Prec@1 65.000 (59.655)
Epoch: [2][8100/12812]	Time 0.212 (0.208)	Data 0.001 (0.001)	Loss 3.0835 (2.8245)	Prec@1 52.000 (59.658)
Epoch: [2][8150/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 2.8002 (2.8248)	Prec@1 61.000 (59.653)
Epoch: [2][8200/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6594 (2.8248)	Prec@1 66.000 (59.655)
Epoch: [2][8250/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6726 (2.8248)	Prec@1 66.000 (59.652)
Epoch: [2][8300/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9140 (2.8248)	Prec@1 53.000 (59.652)
Epoch: [2][8350/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.9235 (2.8249)	Prec@1 54.000 (59.650)
Epoch: [2][8400/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.2544 (2.8251)	Prec@1 53.000 (59.649)
Epoch: [2][8450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8481 (2.8251)	Prec@1 59.000 (59.652)
Epoch: [2][8500/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0173 (2.8252)	Prec@1 56.000 (59.643)
Epoch: [2][8550/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9751 (2.8253)	Prec@1 56.000 (59.643)
Epoch: [2][8600/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.6028 (2.8251)	Prec@1 67.000 (59.648)
Epoch: [2][8650/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.7611 (2.8251)	Prec@1 61.000 (59.647)
Epoch: [2][8700/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7048 (2.8251)	Prec@1 59.000 (59.643)
Epoch: [2][8750/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9085 (2.8251)	Prec@1 61.000 (59.645)
Epoch: [2][8800/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7574 (2.8250)	Prec@1 67.000 (59.649)
Epoch: [2][8850/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9521 (2.8249)	Prec@1 56.000 (59.656)
Epoch: [2][8900/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7947 (2.8251)	Prec@1 56.000 (59.649)
Epoch: [2][8950/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.7518 (2.8251)	Prec@1 63.000 (59.650)
Epoch: [2][9000/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9092 (2.8250)	Prec@1 57.000 (59.652)
Epoch: [2][9050/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.8148 (2.8250)	Prec@1 59.000 (59.652)
Epoch: [2][9100/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9663 (2.8249)	Prec@1 56.000 (59.657)
Epoch: [2][9150/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7917 (2.8248)	Prec@1 67.000 (59.653)
Epoch: [2][9200/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.5888 (2.8249)	Prec@1 64.000 (59.649)
Epoch: [2][9250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9054 (2.8250)	Prec@1 62.000 (59.652)
Epoch: [2][9300/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6802 (2.8250)	Prec@1 61.000 (59.653)
Epoch: [2][9350/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8488 (2.8250)	Prec@1 59.000 (59.652)
Epoch: [2][9400/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6621 (2.8251)	Prec@1 64.000 (59.647)
Epoch: [2][9450/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.8801 (2.8253)	Prec@1 54.000 (59.643)
Epoch: [2][9500/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.8172 (2.8254)	Prec@1 59.000 (59.644)
Epoch: [2][9550/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8421 (2.8253)	Prec@1 57.000 (59.644)
Epoch: [2][9600/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7069 (2.8252)	Prec@1 66.000 (59.649)
Epoch: [2][9650/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7772 (2.8252)	Prec@1 61.000 (59.645)
Epoch: [2][9700/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6826 (2.8252)	Prec@1 56.000 (59.649)
Epoch: [2][9750/12812]	Time 0.211 (0.208)	Data 0.001 (0.001)	Loss 2.7525 (2.8253)	Prec@1 61.000 (59.650)
Epoch: [2][9800/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8226 (2.8254)	Prec@1 63.000 (59.646)
Epoch: [2][9850/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8807 (2.8254)	Prec@1 56.000 (59.650)
Epoch: [2][9900/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9284 (2.8254)	Prec@1 59.000 (59.649)
Epoch: [2][9950/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 3.0333 (2.8257)	Prec@1 50.000 (59.641)
Epoch: [2][10000/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0232 (2.8260)	Prec@1 54.000 (59.635)
Epoch: [2][10050/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 2.8045 (2.8259)	Prec@1 61.000 (59.635)
Epoch: [2][10100/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 3.1176 (2.8260)	Prec@1 52.000 (59.633)
Epoch: [2][10150/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.5092 (2.8258)	Prec@1 71.000 (59.639)
Epoch: [2][10200/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 3.0389 (2.8260)	Prec@1 55.000 (59.636)
Epoch: [2][10250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8829 (2.8259)	Prec@1 53.000 (59.636)
Epoch: [2][10300/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.5545 (2.8260)	Prec@1 68.000 (59.634)
Epoch: [2][10350/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8244 (2.8259)	Prec@1 59.000 (59.636)
Epoch: [2][10400/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 3.1527 (2.8258)	Prec@1 53.000 (59.638)
Epoch: [2][10450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7763 (2.8258)	Prec@1 56.000 (59.638)
Epoch: [2][10500/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.6543 (2.8257)	Prec@1 64.000 (59.642)
Epoch: [2][10550/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9398 (2.8258)	Prec@1 58.000 (59.643)
Epoch: [2][10600/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.7518 (2.8258)	Prec@1 60.000 (59.645)
Epoch: [2][10650/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.7919 (2.8259)	Prec@1 59.000 (59.646)
Epoch: [2][10700/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 2.8187 (2.8259)	Prec@1 57.000 (59.645)
Epoch: [2][10750/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 3.1152 (2.8260)	Prec@1 48.000 (59.641)
Epoch: [2][10800/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8194 (2.8259)	Prec@1 58.000 (59.641)
Epoch: [2][10850/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.5087 (2.8260)	Prec@1 71.000 (59.639)
Epoch: [2][10900/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.5477 (2.8260)	Prec@1 72.000 (59.638)
Epoch: [2][10950/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.8239 (2.8261)	Prec@1 59.000 (59.635)
Epoch: [2][11000/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9656 (2.8262)	Prec@1 54.000 (59.634)
Epoch: [2][11050/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0011 (2.8263)	Prec@1 54.000 (59.632)
Epoch: [2][11100/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7816 (2.8262)	Prec@1 65.000 (59.635)
Epoch: [2][11150/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7999 (2.8262)	Prec@1 56.000 (59.635)
Epoch: [2][11200/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7231 (2.8261)	Prec@1 61.000 (59.639)
Epoch: [2][11250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7468 (2.8263)	Prec@1 61.000 (59.635)
Epoch: [2][11300/12812]	Time 0.211 (0.208)	Data 0.001 (0.001)	Loss 3.0346 (2.8263)	Prec@1 57.000 (59.633)
Epoch: [2][11350/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 2.8770 (2.8264)	Prec@1 60.000 (59.636)
Epoch: [2][11400/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.5827 (2.8264)	Prec@1 67.000 (59.632)
Epoch: [2][11450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8412 (2.8265)	Prec@1 56.000 (59.628)
Epoch: [2][11500/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7525 (2.8265)	Prec@1 59.000 (59.624)
Epoch: [2][11550/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9012 (2.8265)	Prec@1 56.000 (59.625)
Epoch: [2][11600/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7638 (2.8267)	Prec@1 58.000 (59.617)
Epoch: [2][11650/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8041 (2.8266)	Prec@1 62.000 (59.618)
Epoch: [2][11700/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.7387 (2.8266)	Prec@1 66.000 (59.619)
Epoch: [2][11750/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6172 (2.8266)	Prec@1 62.000 (59.619)
Epoch: [2][11800/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.8143 (2.8267)	Prec@1 61.000 (59.619)
Epoch: [2][11850/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9512 (2.8268)	Prec@1 55.000 (59.616)
Epoch: [2][11900/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9567 (2.8269)	Prec@1 57.000 (59.619)
Epoch: [2][11950/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0295 (2.8270)	Prec@1 47.000 (59.618)
Epoch: [2][12000/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9745 (2.8270)	Prec@1 60.000 (59.620)
Epoch: [2][12050/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.7235 (2.8270)	Prec@1 61.000 (59.621)
Epoch: [2][12100/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.5953 (2.8272)	Prec@1 65.000 (59.621)
Epoch: [2][12150/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7838 (2.8272)	Prec@1 61.000 (59.623)
Epoch: [2][12200/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9246 (2.8272)	Prec@1 59.000 (59.621)
Epoch: [2][12250/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.7119 (2.8274)	Prec@1 62.000 (59.613)
Epoch: [2][12300/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9043 (2.8274)	Prec@1 61.000 (59.612)
Epoch: [2][12350/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 2.9522 (2.8276)	Prec@1 60.000 (59.606)
Epoch: [2][12400/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7499 (2.8276)	Prec@1 60.000 (59.609)
Epoch: [2][12450/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9873 (2.8277)	Prec@1 51.000 (59.609)
Epoch: [2][12500/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0967 (2.8279)	Prec@1 57.000 (59.605)
Epoch: [2][12550/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8539 (2.8279)	Prec@1 61.000 (59.605)
Epoch: [2][12600/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8906 (2.8278)	Prec@1 57.000 (59.605)
Epoch: [2][12650/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.1232 (2.8280)	Prec@1 52.000 (59.599)
Epoch: [2][12700/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.5840 (2.8281)	Prec@1 67.000 (59.595)
Epoch: [2][12750/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 2.7686 (2.8281)	Prec@1 62.000 (59.595)
Epoch: [2][12800/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7786 (2.8281)	Prec@1 64.000 (59.596)
Test: [0/500]	Time 0.056 (0.056)	Loss 0.9421 (0.9421)	Prec@1 88.000 (88.000)
Test: [50/500]	Time 0.057 (0.057)	Loss 1.0660 (1.3250)	Prec@1 80.000 (76.510)
Test: [100/500]	Time 0.056 (0.057)	Loss 1.8683 (1.4114)	Prec@1 49.000 (73.396)
Test: [150/500]	Time 0.056 (0.057)	Loss 0.8533 (1.4118)	Prec@1 89.000 (73.000)
Test: [200/500]	Time 0.057 (0.057)	Loss 2.0443 (1.4115)	Prec@1 59.000 (73.179)
Test: [250/500]	Time 0.060 (0.057)	Loss 2.2978 (1.5448)	Prec@1 54.000 (70.319)
Test: [300/500]	Time 0.063 (0.058)	Loss 3.3076 (1.6184)	Prec@1 39.000 (68.864)
Test: [350/500]	Time 0.059 (0.058)	Loss 1.6945 (1.6797)	Prec@1 65.000 (67.470)
Test: [400/500]	Time 0.058 (0.059)	Loss 1.0856 (1.7341)	Prec@1 79.000 (66.401)
Test: [450/500]	Time 0.062 (0.059)	Loss 1.5547 (1.7819)	Prec@1 74.000 (65.370)
 * Prec@1 65.662
current lr 1.00000e-04
Epoch: [3][0/12812]	Time 0.204 (0.204)	Data 0.001 (0.001)	Loss 2.8193 (2.8193)	Prec@1 58.000 (58.000)
Epoch: [3][50/12812]	Time 0.205 (0.205)	Data 0.000 (0.001)	Loss 2.8435 (2.8331)	Prec@1 61.000 (59.216)
Epoch: [3][100/12812]	Time 0.207 (0.206)	Data 0.001 (0.001)	Loss 2.7718 (2.8401)	Prec@1 61.000 (59.386)
Epoch: [3][150/12812]	Time 0.207 (0.206)	Data 0.001 (0.001)	Loss 2.5973 (2.8244)	Prec@1 66.000 (59.828)
Epoch: [3][200/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9967 (2.8365)	Prec@1 60.000 (59.662)
Epoch: [3][250/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.5917 (2.8321)	Prec@1 68.000 (59.693)
Epoch: [3][300/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.2056 (2.8335)	Prec@1 52.000 (59.784)
Epoch: [3][350/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7646 (2.8317)	Prec@1 60.000 (59.635)
Epoch: [3][400/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7108 (2.8295)	Prec@1 63.000 (59.771)
Epoch: [3][450/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9019 (2.8326)	Prec@1 60.000 (59.652)
Epoch: [3][500/12812]	Time 0.209 (0.207)	Data 0.000 (0.001)	Loss 2.7624 (2.8336)	Prec@1 64.000 (59.629)
Epoch: [3][550/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8710 (2.8347)	Prec@1 62.000 (59.610)
Epoch: [3][600/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8105 (2.8384)	Prec@1 67.000 (59.514)
Epoch: [3][650/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8458 (2.8390)	Prec@1 56.000 (59.499)
Epoch: [3][700/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.7518 (2.8388)	Prec@1 64.000 (59.498)
Epoch: [3][750/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6252 (2.8415)	Prec@1 68.000 (59.449)
Epoch: [3][800/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6985 (2.8404)	Prec@1 65.000 (59.453)
Epoch: [3][850/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.7059 (2.8388)	Prec@1 67.000 (59.528)
Epoch: [3][900/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7923 (2.8394)	Prec@1 61.000 (59.522)
Epoch: [3][950/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9846 (2.8409)	Prec@1 58.000 (59.461)
Epoch: [3][1000/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8604 (2.8413)	Prec@1 56.000 (59.442)
Epoch: [3][1050/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8144 (2.8418)	Prec@1 57.000 (59.420)
Epoch: [3][1100/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0033 (2.8428)	Prec@1 52.000 (59.381)
Epoch: [3][1150/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8119 (2.8418)	Prec@1 59.000 (59.425)
Epoch: [3][1200/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6203 (2.8425)	Prec@1 65.000 (59.401)
Epoch: [3][1250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8634 (2.8425)	Prec@1 62.000 (59.446)
Epoch: [3][1300/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.5926 (2.8413)	Prec@1 62.000 (59.455)
Epoch: [3][1350/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9326 (2.8425)	Prec@1 54.000 (59.445)
Epoch: [3][1400/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9167 (2.8424)	Prec@1 55.000 (59.418)
Epoch: [3][1450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8749 (2.8430)	Prec@1 57.000 (59.397)
Epoch: [3][1500/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.7349 (2.8423)	Prec@1 62.000 (59.401)
Epoch: [3][1550/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0666 (2.8431)	Prec@1 53.000 (59.377)
Epoch: [3][1600/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0064 (2.8432)	Prec@1 51.000 (59.380)
Epoch: [3][1650/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6503 (2.8421)	Prec@1 68.000 (59.394)
Epoch: [3][1700/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7398 (2.8413)	Prec@1 66.000 (59.415)
Epoch: [3][1750/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8417 (2.8420)	Prec@1 67.000 (59.412)
Epoch: [3][1800/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7364 (2.8414)	Prec@1 57.000 (59.439)
Epoch: [3][1850/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7695 (2.8418)	Prec@1 60.000 (59.433)
Epoch: [3][1900/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8383 (2.8413)	Prec@1 63.000 (59.457)
Epoch: [3][1950/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7298 (2.8417)	Prec@1 63.000 (59.453)
Epoch: [3][2000/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8584 (2.8413)	Prec@1 64.000 (59.453)
Epoch: [3][2050/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9348 (2.8416)	Prec@1 54.000 (59.438)
Epoch: [3][2100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6764 (2.8414)	Prec@1 59.000 (59.441)
Epoch: [3][2150/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9566 (2.8410)	Prec@1 63.000 (59.461)
Epoch: [3][2200/12812]	Time 0.318 (0.208)	Data 0.117 (0.001)	Loss 2.6848 (2.8411)	Prec@1 64.000 (59.480)
Epoch: [3][2250/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.8269 (2.8406)	Prec@1 59.000 (59.510)
Epoch: [3][2300/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9290 (2.8406)	Prec@1 59.000 (59.508)
Epoch: [3][2350/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.7347 (2.8409)	Prec@1 64.000 (59.511)
Epoch: [3][2400/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 3.0817 (2.8407)	Prec@1 50.000 (59.514)
Epoch: [3][2450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7339 (2.8412)	Prec@1 57.000 (59.497)
Epoch: [3][2500/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9949 (2.8411)	Prec@1 56.000 (59.499)
Epoch: [3][2550/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.8367 (2.8412)	Prec@1 60.000 (59.496)
Epoch: [3][2600/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.9674 (2.8416)	Prec@1 63.000 (59.505)
Epoch: [3][2650/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8832 (2.8409)	Prec@1 52.000 (59.527)
Epoch: [3][2700/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8126 (2.8411)	Prec@1 54.000 (59.513)
Epoch: [3][2750/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9451 (2.8412)	Prec@1 61.000 (59.511)
Epoch: [3][2800/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6043 (2.8416)	Prec@1 62.000 (59.498)
Epoch: [3][2850/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9900 (2.8417)	Prec@1 59.000 (59.482)
Epoch: [3][2900/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6359 (2.8422)	Prec@1 66.000 (59.463)
Epoch: [3][2950/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.1576 (2.8419)	Prec@1 52.000 (59.461)
Epoch: [3][3000/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8851 (2.8418)	Prec@1 58.000 (59.470)
Epoch: [3][3050/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8112 (2.8412)	Prec@1 57.000 (59.491)
Epoch: [3][3100/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8747 (2.8418)	Prec@1 59.000 (59.474)
Epoch: [3][3150/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 2.6707 (2.8421)	Prec@1 60.000 (59.459)
Epoch: [3][3200/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 2.7737 (2.8417)	Prec@1 65.000 (59.472)
Epoch: [3][3250/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7514 (2.8421)	Prec@1 62.000 (59.469)
Epoch: [3][3300/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7892 (2.8416)	Prec@1 59.000 (59.475)
Epoch: [3][3350/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8131 (2.8419)	Prec@1 57.000 (59.474)
Epoch: [3][3400/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0160 (2.8421)	Prec@1 55.000 (59.471)
Epoch: [3][3450/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7630 (2.8419)	Prec@1 60.000 (59.460)
Epoch: [3][3500/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7956 (2.8422)	Prec@1 60.000 (59.450)
Epoch: [3][3550/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7698 (2.8426)	Prec@1 58.000 (59.438)
Epoch: [3][3600/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9794 (2.8420)	Prec@1 56.000 (59.456)
Epoch: [3][3650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8723 (2.8418)	Prec@1 59.000 (59.456)
Epoch: [3][3700/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6143 (2.8416)	Prec@1 63.000 (59.449)
Epoch: [3][3750/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0969 (2.8419)	Prec@1 58.000 (59.440)
Epoch: [3][3800/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9159 (2.8422)	Prec@1 57.000 (59.439)
Epoch: [3][3850/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8538 (2.8419)	Prec@1 56.000 (59.441)
Epoch: [3][3900/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 3.0264 (2.8425)	Prec@1 53.000 (59.424)
Epoch: [3][3950/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6425 (2.8422)	Prec@1 69.000 (59.437)
Epoch: [3][4000/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6566 (2.8420)	Prec@1 64.000 (59.435)
Epoch: [3][4050/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8370 (2.8418)	Prec@1 62.000 (59.441)
Epoch: [3][4100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0066 (2.8424)	Prec@1 59.000 (59.424)
Epoch: [3][4150/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9719 (2.8425)	Prec@1 54.000 (59.413)
Epoch: [3][4200/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7586 (2.8429)	Prec@1 58.000 (59.402)
Epoch: [3][4250/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 2.9102 (2.8434)	Prec@1 62.000 (59.401)
Epoch: [3][4300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8547 (2.8439)	Prec@1 64.000 (59.391)
Epoch: [3][4350/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 2.9424 (2.8437)	Prec@1 53.000 (59.397)
Epoch: [3][4400/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8926 (2.8436)	Prec@1 55.000 (59.397)
Epoch: [3][4450/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7800 (2.8437)	Prec@1 63.000 (59.394)
Epoch: [3][4500/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7176 (2.8437)	Prec@1 61.000 (59.393)
Epoch: [3][4550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.5068 (2.8436)	Prec@1 66.000 (59.396)
Epoch: [3][4600/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9768 (2.8436)	Prec@1 53.000 (59.398)
Epoch: [3][4650/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6390 (2.8438)	Prec@1 67.000 (59.388)
Epoch: [3][4700/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9858 (2.8438)	Prec@1 57.000 (59.389)
Epoch: [3][4750/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7895 (2.8438)	Prec@1 64.000 (59.388)
Epoch: [3][4800/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9366 (2.8439)	Prec@1 59.000 (59.387)
Epoch: [3][4850/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.5820 (2.8436)	Prec@1 64.000 (59.389)
Epoch: [3][4900/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 2.7314 (2.8441)	Prec@1 65.000 (59.386)
Epoch: [3][4950/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8172 (2.8442)	Prec@1 58.000 (59.384)
Epoch: [3][5000/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8588 (2.8443)	Prec@1 63.000 (59.381)
Epoch: [3][5050/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9464 (2.8445)	Prec@1 51.000 (59.378)
Epoch: [3][5100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6421 (2.8448)	Prec@1 62.000 (59.370)
Epoch: [3][5150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8882 (2.8450)	Prec@1 63.000 (59.366)
Epoch: [3][5200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8872 (2.8453)	Prec@1 56.000 (59.361)
Epoch: [3][5250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7840 (2.8453)	Prec@1 56.000 (59.364)
Epoch: [3][5300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8982 (2.8452)	Prec@1 63.000 (59.368)
Epoch: [3][5350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7626 (2.8451)	Prec@1 59.000 (59.370)
Epoch: [3][5400/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8731 (2.8455)	Prec@1 59.000 (59.367)
Epoch: [3][5450/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 2.4601 (2.8452)	Prec@1 62.000 (59.381)
Epoch: [3][5500/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8400 (2.8451)	Prec@1 59.000 (59.390)
Epoch: [3][5550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6713 (2.8448)	Prec@1 63.000 (59.400)
Epoch: [3][5600/12812]	Time 0.207 (0.207)	Data 0.000 (0.001)	Loss 2.6769 (2.8447)	Prec@1 61.000 (59.406)
Epoch: [3][5650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6672 (2.8449)	Prec@1 57.000 (59.394)
Epoch: [3][5700/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.1870 (2.8452)	Prec@1 58.000 (59.386)
Epoch: [3][5750/12812]	Time 0.208 (0.207)	Data 0.000 (0.001)	Loss 2.9944 (2.8451)	Prec@1 55.000 (59.388)
Epoch: [3][5800/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.0520 (2.8454)	Prec@1 53.000 (59.385)
Epoch: [3][5850/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.0225 (2.8456)	Prec@1 60.000 (59.383)
Epoch: [3][5900/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7706 (2.8456)	Prec@1 60.000 (59.389)
Epoch: [3][5950/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9079 (2.8458)	Prec@1 60.000 (59.388)
Epoch: [3][6000/12812]	Time 0.207 (0.207)	Data 0.000 (0.001)	Loss 2.7677 (2.8458)	Prec@1 66.000 (59.385)
Epoch: [3][6050/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7155 (2.8458)	Prec@1 63.000 (59.386)
Epoch: [3][6100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7964 (2.8458)	Prec@1 58.000 (59.387)
Epoch: [3][6150/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 3.2012 (2.8458)	Prec@1 51.000 (59.389)
Epoch: [3][6200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8562 (2.8457)	Prec@1 59.000 (59.392)
Epoch: [3][6250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7712 (2.8459)	Prec@1 64.000 (59.389)
Epoch: [3][6300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9377 (2.8460)	Prec@1 53.000 (59.385)
Epoch: [3][6350/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8160 (2.8459)	Prec@1 65.000 (59.392)
Epoch: [3][6400/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6885 (2.8462)	Prec@1 64.000 (59.383)
Epoch: [3][6450/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7041 (2.8462)	Prec@1 68.000 (59.383)
Epoch: [3][6500/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7708 (2.8462)	Prec@1 60.000 (59.383)
Epoch: [3][6550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8338 (2.8462)	Prec@1 60.000 (59.386)
Epoch: [3][6600/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0176 (2.8464)	Prec@1 51.000 (59.379)
Epoch: [3][6650/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7961 (2.8466)	Prec@1 58.000 (59.373)
Epoch: [3][6700/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8024 (2.8467)	Prec@1 61.000 (59.372)
Epoch: [3][6750/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9218 (2.8468)	Prec@1 54.000 (59.368)
Epoch: [3][6800/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8217 (2.8469)	Prec@1 60.000 (59.367)
Epoch: [3][6850/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 2.6575 (2.8469)	Prec@1 68.000 (59.370)
Epoch: [3][6900/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.5453 (2.8471)	Prec@1 70.000 (59.359)
Epoch: [3][6950/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6586 (2.8472)	Prec@1 65.000 (59.350)
Epoch: [3][7000/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9572 (2.8473)	Prec@1 60.000 (59.353)
Epoch: [3][7050/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9407 (2.8477)	Prec@1 60.000 (59.347)
Epoch: [3][7100/12812]	Time 0.210 (0.207)	Data 0.001 (0.001)	Loss 2.8322 (2.8479)	Prec@1 66.000 (59.349)
Epoch: [3][7150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8796 (2.8478)	Prec@1 57.000 (59.355)
Epoch: [3][7200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6274 (2.8477)	Prec@1 62.000 (59.358)
Epoch: [3][7250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9240 (2.8477)	Prec@1 58.000 (59.361)
Epoch: [3][7300/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8028 (2.8475)	Prec@1 61.000 (59.365)
Epoch: [3][7350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7778 (2.8478)	Prec@1 60.000 (59.359)
Epoch: [3][7400/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7506 (2.8477)	Prec@1 63.000 (59.363)
Epoch: [3][7450/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6926 (2.8477)	Prec@1 58.000 (59.362)
Epoch: [3][7500/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6146 (2.8478)	Prec@1 64.000 (59.358)
Epoch: [3][7550/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9169 (2.8479)	Prec@1 59.000 (59.356)
Epoch: [3][7600/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.0762 (2.8480)	Prec@1 58.000 (59.353)
Epoch: [3][7650/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7242 (2.8482)	Prec@1 61.000 (59.351)
Epoch: [3][7700/12812]	Time 0.210 (0.207)	Data 0.001 (0.001)	Loss 2.8535 (2.8482)	Prec@1 60.000 (59.351)
Epoch: [3][7750/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9045 (2.8483)	Prec@1 62.000 (59.352)
Epoch: [3][7800/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9456 (2.8486)	Prec@1 58.000 (59.338)
Epoch: [3][7850/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8809 (2.8486)	Prec@1 62.000 (59.341)
Epoch: [3][7900/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 3.1271 (2.8490)	Prec@1 48.000 (59.337)
Epoch: [3][7950/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.6028 (2.8490)	Prec@1 63.000 (59.341)
Epoch: [3][8000/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7914 (2.8490)	Prec@1 65.000 (59.345)
Epoch: [3][8050/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8295 (2.8492)	Prec@1 61.000 (59.340)
Epoch: [3][8100/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7142 (2.8494)	Prec@1 61.000 (59.336)
Epoch: [3][8150/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8856 (2.8495)	Prec@1 66.000 (59.336)
Epoch: [3][8200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7396 (2.8498)	Prec@1 64.000 (59.328)
Epoch: [3][8250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9218 (2.8499)	Prec@1 57.000 (59.327)
Epoch: [3][8300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9475 (2.8500)	Prec@1 56.000 (59.324)
Epoch: [3][8350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7702 (2.8502)	Prec@1 65.000 (59.328)
Epoch: [3][8400/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8219 (2.8503)	Prec@1 62.000 (59.326)
Epoch: [3][8450/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.6543 (2.8506)	Prec@1 60.000 (59.318)
Epoch: [3][8500/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8600 (2.8509)	Prec@1 59.000 (59.307)
Epoch: [3][8550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0833 (2.8511)	Prec@1 62.000 (59.306)
Epoch: [3][8600/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.6829 (2.8510)	Prec@1 64.000 (59.307)
Epoch: [3][8650/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9484 (2.8510)	Prec@1 57.000 (59.308)
Epoch: [3][8700/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.1053 (2.8510)	Prec@1 51.000 (59.308)
Epoch: [3][8750/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9766 (2.8510)	Prec@1 59.000 (59.315)
Epoch: [3][8800/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8283 (2.8510)	Prec@1 61.000 (59.316)
Epoch: [3][8850/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6191 (2.8511)	Prec@1 68.000 (59.318)
Epoch: [3][8900/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8350 (2.8514)	Prec@1 61.000 (59.314)
Epoch: [3][8950/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.1275 (2.8514)	Prec@1 55.000 (59.315)
Epoch: [3][9000/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9046 (2.8513)	Prec@1 54.000 (59.318)
Epoch: [3][9050/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.0143 (2.8513)	Prec@1 55.000 (59.318)
Epoch: [3][9100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6670 (2.8513)	Prec@1 65.000 (59.317)
Epoch: [3][9150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7113 (2.8513)	Prec@1 64.000 (59.320)
Epoch: [3][9200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6806 (2.8514)	Prec@1 65.000 (59.324)
Epoch: [3][9250/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.6477 (2.8515)	Prec@1 58.000 (59.322)
Epoch: [3][9300/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0191 (2.8517)	Prec@1 56.000 (59.321)
Epoch: [3][9350/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8054 (2.8515)	Prec@1 59.000 (59.327)
Epoch: [3][9400/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9369 (2.8516)	Prec@1 60.000 (59.326)
Epoch: [3][9450/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7952 (2.8518)	Prec@1 66.000 (59.328)
Epoch: [3][9500/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 2.9416 (2.8519)	Prec@1 58.000 (59.324)
Epoch: [3][9550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8918 (2.8520)	Prec@1 61.000 (59.324)
Epoch: [3][9600/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7985 (2.8520)	Prec@1 56.000 (59.322)
Epoch: [3][9650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9026 (2.8521)	Prec@1 64.000 (59.321)
Epoch: [3][9700/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8094 (2.8521)	Prec@1 51.000 (59.321)
Epoch: [3][9750/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.1209 (2.8523)	Prec@1 54.000 (59.318)
Epoch: [3][9800/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8983 (2.8524)	Prec@1 56.000 (59.316)
Epoch: [3][9850/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9898 (2.8526)	Prec@1 58.000 (59.312)
Epoch: [3][9900/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0069 (2.8527)	Prec@1 54.000 (59.309)
Epoch: [3][9950/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9819 (2.8529)	Prec@1 58.000 (59.304)
Epoch: [3][10000/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6538 (2.8532)	Prec@1 68.000 (59.302)
Epoch: [3][10050/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0181 (2.8532)	Prec@1 55.000 (59.300)
Epoch: [3][10100/12812]	Time 0.207 (0.207)	Data 0.000 (0.001)	Loss 2.8546 (2.8533)	Prec@1 57.000 (59.299)
Epoch: [3][10150/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 3.1204 (2.8534)	Prec@1 50.000 (59.296)
Epoch: [3][10200/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9000 (2.8535)	Prec@1 57.000 (59.299)
Epoch: [3][10250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7696 (2.8536)	Prec@1 62.000 (59.301)
Epoch: [3][10300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.5848 (2.8535)	Prec@1 67.000 (59.299)
Epoch: [3][10350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0135 (2.8536)	Prec@1 52.000 (59.297)
Epoch: [3][10400/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8313 (2.8535)	Prec@1 60.000 (59.296)
Epoch: [3][10450/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.1106 (2.8536)	Prec@1 54.000 (59.293)
Epoch: [3][10500/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9195 (2.8536)	Prec@1 60.000 (59.294)
Epoch: [3][10550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7104 (2.8538)	Prec@1 59.000 (59.290)
Epoch: [3][10600/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7410 (2.8539)	Prec@1 63.000 (59.287)
Epoch: [3][10650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7622 (2.8539)	Prec@1 66.000 (59.287)
Epoch: [3][10700/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8185 (2.8539)	Prec@1 51.000 (59.288)
Epoch: [3][10750/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8913 (2.8541)	Prec@1 58.000 (59.283)
Epoch: [3][10800/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0473 (2.8540)	Prec@1 54.000 (59.285)
Epoch: [3][10850/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8048 (2.8541)	Prec@1 65.000 (59.280)
Epoch: [3][10900/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6090 (2.8541)	Prec@1 67.000 (59.282)
Epoch: [3][10950/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9080 (2.8543)	Prec@1 63.000 (59.279)
Epoch: [3][11000/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8884 (2.8543)	Prec@1 55.000 (59.277)
Epoch: [3][11050/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0739 (2.8545)	Prec@1 61.000 (59.277)
Epoch: [3][11100/12812]	Time 0.206 (0.207)	Data 0.002 (0.001)	Loss 2.8594 (2.8544)	Prec@1 57.000 (59.279)
Epoch: [3][11150/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.6711 (2.8545)	Prec@1 65.000 (59.280)
Epoch: [3][11200/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0321 (2.8547)	Prec@1 52.000 (59.276)
Epoch: [3][11250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9685 (2.8549)	Prec@1 53.000 (59.275)
Epoch: [3][11300/12812]	Time 0.210 (0.207)	Data 0.001 (0.001)	Loss 2.6680 (2.8549)	Prec@1 68.000 (59.280)
Epoch: [3][11350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8167 (2.8550)	Prec@1 61.000 (59.278)
Epoch: [3][11400/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9488 (2.8550)	Prec@1 57.000 (59.278)
Epoch: [3][11450/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9409 (2.8552)	Prec@1 52.000 (59.273)
Epoch: [3][11500/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7735 (2.8554)	Prec@1 63.000 (59.267)
Epoch: [3][11550/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 2.7678 (2.8553)	Prec@1 61.000 (59.268)
Epoch: [3][11600/12812]	Time 0.207 (0.207)	Data 0.000 (0.001)	Loss 2.5841 (2.8554)	Prec@1 62.000 (59.262)
Epoch: [3][11650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6259 (2.8555)	Prec@1 60.000 (59.259)
Epoch: [3][11700/12812]	Time 0.206 (0.207)	Data 0.000 (0.001)	Loss 3.1075 (2.8557)	Prec@1 52.000 (59.257)
Epoch: [3][11750/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8056 (2.8558)	Prec@1 62.000 (59.256)
Epoch: [3][11800/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7595 (2.8557)	Prec@1 62.000 (59.258)
Epoch: [3][11850/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9592 (2.8558)	Prec@1 60.000 (59.258)
Epoch: [3][11900/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9027 (2.8559)	Prec@1 57.000 (59.260)
Epoch: [3][11950/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 2.9580 (2.8559)	Prec@1 51.000 (59.259)
Epoch: [3][12000/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8354 (2.8559)	Prec@1 62.000 (59.261)
Epoch: [3][12050/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 2.7993 (2.8561)	Prec@1 58.000 (59.258)
Epoch: [3][12100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.5780 (2.8561)	Prec@1 64.000 (59.259)
Epoch: [3][12150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7481 (2.8562)	Prec@1 64.000 (59.257)
Epoch: [3][12200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0364 (2.8563)	Prec@1 58.000 (59.254)
Epoch: [3][12250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9597 (2.8565)	Prec@1 56.000 (59.248)
Epoch: [3][12300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8564 (2.8565)	Prec@1 64.000 (59.242)
Epoch: [3][12350/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7148 (2.8567)	Prec@1 67.000 (59.241)
Epoch: [3][12400/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 2.7917 (2.8568)	Prec@1 64.000 (59.238)
Epoch: [3][12450/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9931 (2.8569)	Prec@1 58.000 (59.239)
Epoch: [3][12500/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0460 (2.8571)	Prec@1 58.000 (59.234)
Epoch: [3][12550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6890 (2.8572)	Prec@1 71.000 (59.230)
Epoch: [3][12600/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.1490 (2.8572)	Prec@1 52.000 (59.231)
Epoch: [3][12650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.3958 (2.8574)	Prec@1 44.000 (59.225)
Epoch: [3][12700/12812]	Time 0.207 (0.207)	Data 0.000 (0.001)	Loss 2.6374 (2.8575)	Prec@1 67.000 (59.221)
Epoch: [3][12750/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7974 (2.8576)	Prec@1 67.000 (59.220)
Epoch: [3][12800/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0493 (2.8577)	Prec@1 52.000 (59.215)
Test: [0/500]	Time 0.056 (0.056)	Loss 0.9614 (0.9614)	Prec@1 89.000 (89.000)
Test: [50/500]	Time 0.056 (0.057)	Loss 1.0853 (1.3605)	Prec@1 80.000 (75.941)
Test: [100/500]	Time 0.056 (0.057)	Loss 1.8800 (1.4468)	Prec@1 50.000 (72.802)
Test: [150/500]	Time 0.057 (0.057)	Loss 0.8839 (1.4424)	Prec@1 89.000 (72.589)
Test: [200/500]	Time 0.056 (0.057)	Loss 2.0819 (1.4439)	Prec@1 58.000 (72.791)
Test: [250/500]	Time 0.056 (0.057)	Loss 2.3404 (1.5803)	Prec@1 54.000 (69.920)
Test: [300/500]	Time 0.056 (0.057)	Loss 3.4055 (1.6547)	Prec@1 36.000 (68.445)
Test: [350/500]	Time 0.056 (0.057)	Loss 1.7273 (1.7175)	Prec@1 66.000 (66.991)
Test: [400/500]	Time 0.057 (0.057)	Loss 1.1413 (1.7728)	Prec@1 79.000 (65.960)
Test: [450/500]	Time 0.056 (0.057)	Loss 1.6180 (1.8210)	Prec@1 74.000 (64.951)
 * Prec@1 65.248
current lr 1.00000e-04
Epoch: [4][0/12812]	Time 0.203 (0.203)	Data 0.001 (0.001)	Loss 2.9279 (2.9279)	Prec@1 59.000 (59.000)
Epoch: [4][50/12812]	Time 0.206 (0.206)	Data 0.001 (0.001)	Loss 3.0918 (2.8544)	Prec@1 53.000 (58.961)
Epoch: [4][100/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7838 (2.8779)	Prec@1 60.000 (58.871)
Epoch: [4][150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8610 (2.8721)	Prec@1 55.000 (58.954)
Epoch: [4][200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6791 (2.8752)	Prec@1 56.000 (58.876)
Epoch: [4][250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9439 (2.8761)	Prec@1 56.000 (58.817)
Epoch: [4][300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7121 (2.8709)	Prec@1 60.000 (59.030)
Epoch: [4][350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8978 (2.8710)	Prec@1 59.000 (59.017)
Epoch: [4][400/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8533 (2.8686)	Prec@1 63.000 (59.060)
Epoch: [4][450/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7973 (2.8724)	Prec@1 63.000 (58.920)
Epoch: [4][500/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.2132 (2.8744)	Prec@1 46.000 (58.882)
Epoch: [4][550/12812]	Time 0.207 (0.207)	Data 0.000 (0.001)	Loss 2.8022 (2.8728)	Prec@1 63.000 (58.880)
Epoch: [4][600/12812]	Time 0.260 (0.208)	Data 0.060 (0.001)	Loss 3.0048 (2.8776)	Prec@1 56.000 (58.805)
Epoch: [4][650/12812]	Time 0.207 (0.208)	Data 0.001 (0.002)	Loss 2.7790 (2.8777)	Prec@1 62.000 (58.860)
Epoch: [4][700/12812]	Time 0.206 (0.208)	Data 0.001 (0.002)	Loss 2.6730 (2.8782)	Prec@1 67.000 (58.927)
Epoch: [4][750/12812]	Time 0.207 (0.208)	Data 0.001 (0.002)	Loss 2.7022 (2.8791)	Prec@1 65.000 (58.957)
Epoch: [4][800/12812]	Time 0.207 (0.208)	Data 0.001 (0.002)	Loss 2.7003 (2.8781)	Prec@1 66.000 (59.004)
Epoch: [4][850/12812]	Time 0.207 (0.208)	Data 0.000 (0.002)	Loss 2.8567 (2.8764)	Prec@1 58.000 (59.067)
Epoch: [4][900/12812]	Time 0.207 (0.208)	Data 0.001 (0.002)	Loss 3.0933 (2.8770)	Prec@1 51.000 (59.049)
Epoch: [4][950/12812]	Time 0.207 (0.208)	Data 0.001 (0.002)	Loss 2.6968 (2.8783)	Prec@1 63.000 (59.002)
Epoch: [4][1000/12812]	Time 0.211 (0.208)	Data 0.001 (0.002)	Loss 2.6532 (2.8793)	Prec@1 74.000 (58.997)
Epoch: [4][1050/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7001 (2.8807)	Prec@1 58.000 (58.940)
Epoch: [4][1100/12812]	Time 0.207 (0.208)	Data 0.000 (0.001)	Loss 2.6456 (2.8814)	Prec@1 59.000 (58.916)
Epoch: [4][1150/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8622 (2.8824)	Prec@1 62.000 (58.923)
Epoch: [4][1200/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8659 (2.8826)	Prec@1 60.000 (58.901)
Epoch: [4][1250/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7198 (2.8827)	Prec@1 62.000 (58.894)
Epoch: [4][1300/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7648 (2.8820)	Prec@1 56.000 (58.935)
Epoch: [4][1350/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9891 (2.8835)	Prec@1 57.000 (58.899)
Epoch: [4][1400/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.6432 (2.8833)	Prec@1 62.000 (58.905)
Epoch: [4][1450/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7732 (2.8843)	Prec@1 61.000 (58.874)
Epoch: [4][1500/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9862 (2.8830)	Prec@1 56.000 (58.908)
Epoch: [4][1550/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8751 (2.8829)	Prec@1 56.000 (58.914)
Epoch: [4][1600/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0807 (2.8827)	Prec@1 53.000 (58.926)
Epoch: [4][1650/12812]	Time 0.208 (0.208)	Data 0.000 (0.001)	Loss 3.0130 (2.8820)	Prec@1 55.000 (58.954)
Epoch: [4][1700/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.8138 (2.8811)	Prec@1 58.000 (58.968)
Epoch: [4][1750/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 3.0241 (2.8812)	Prec@1 52.000 (58.961)
Epoch: [4][1800/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7698 (2.8817)	Prec@1 65.000 (58.951)
Epoch: [4][1850/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7482 (2.8818)	Prec@1 63.000 (58.922)
Epoch: [4][1900/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8785 (2.8816)	Prec@1 59.000 (58.942)
Epoch: [4][1950/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7984 (2.8816)	Prec@1 61.000 (58.944)
Epoch: [4][2000/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8041 (2.8815)	Prec@1 63.000 (58.947)
Epoch: [4][2050/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8704 (2.8816)	Prec@1 61.000 (58.936)
Epoch: [4][2100/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.1384 (2.8814)	Prec@1 60.000 (58.940)
Epoch: [4][2150/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8747 (2.8813)	Prec@1 63.000 (58.947)
Epoch: [4][2200/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7549 (2.8811)	Prec@1 61.000 (58.946)
Epoch: [4][2250/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.1219 (2.8806)	Prec@1 53.000 (58.975)
Epoch: [4][2300/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7470 (2.8804)	Prec@1 63.000 (58.979)
Epoch: [4][2350/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9788 (2.8812)	Prec@1 59.000 (58.950)
Epoch: [4][2400/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0560 (2.8812)	Prec@1 56.000 (58.953)
Epoch: [4][2450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.9727 (2.8818)	Prec@1 54.000 (58.939)
Epoch: [4][2500/12812]	Time 0.210 (0.208)	Data 0.001 (0.001)	Loss 2.8291 (2.8817)	Prec@1 60.000 (58.948)
Epoch: [4][2550/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6437 (2.8822)	Prec@1 61.000 (58.929)
Epoch: [4][2600/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8837 (2.8825)	Prec@1 64.000 (58.938)
Epoch: [4][2650/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6800 (2.8822)	Prec@1 64.000 (58.950)
Epoch: [4][2700/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0583 (2.8828)	Prec@1 51.000 (58.935)
Epoch: [4][2750/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.6194 (2.8831)	Prec@1 65.000 (58.906)
Epoch: [4][2800/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8375 (2.8839)	Prec@1 60.000 (58.899)
Epoch: [4][2850/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 3.0585 (2.8853)	Prec@1 57.000 (58.865)
Epoch: [4][2900/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.6100 (2.8852)	Prec@1 62.000 (58.852)
Epoch: [4][2950/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.2385 (2.8854)	Prec@1 51.000 (58.837)
Epoch: [4][3000/12812]	Time 0.204 (0.208)	Data 0.001 (0.001)	Loss 2.6905 (2.8850)	Prec@1 64.000 (58.845)
Epoch: [4][3050/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 3.1140 (2.8849)	Prec@1 50.000 (58.841)
Epoch: [4][3100/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8333 (2.8854)	Prec@1 61.000 (58.838)
Epoch: [4][3150/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9919 (2.8858)	Prec@1 59.000 (58.829)
Epoch: [4][3200/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 3.0572 (2.8855)	Prec@1 51.000 (58.832)
Epoch: [4][3250/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.7453 (2.8859)	Prec@1 64.000 (58.823)
Epoch: [4][3300/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8200 (2.8856)	Prec@1 62.000 (58.833)
Epoch: [4][3350/12812]	Time 0.205 (0.208)	Data 0.001 (0.001)	Loss 2.8798 (2.8861)	Prec@1 62.000 (58.829)
Epoch: [4][3400/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.0296 (2.8860)	Prec@1 60.000 (58.834)
Epoch: [4][3450/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 2.6736 (2.8864)	Prec@1 61.000 (58.822)
Epoch: [4][3500/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7801 (2.8871)	Prec@1 65.000 (58.797)
Epoch: [4][3550/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.5740 (2.8871)	Prec@1 65.000 (58.799)
Epoch: [4][3600/12812]	Time 0.208 (0.208)	Data 0.001 (0.001)	Loss 3.1169 (2.8869)	Prec@1 54.000 (58.823)
Epoch: [4][3650/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7685 (2.8868)	Prec@1 60.000 (58.828)
Epoch: [4][3700/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7642 (2.8864)	Prec@1 64.000 (58.837)
Epoch: [4][3750/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.7223 (2.8868)	Prec@1 65.000 (58.822)
Epoch: [4][3800/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9674 (2.8870)	Prec@1 58.000 (58.819)
Epoch: [4][3850/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.9897 (2.8870)	Prec@1 56.000 (58.812)
Epoch: [4][3900/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8530 (2.8877)	Prec@1 54.000 (58.804)
Epoch: [4][3950/12812]	Time 0.207 (0.208)	Data 0.001 (0.001)	Loss 2.8022 (2.8879)	Prec@1 58.000 (58.798)
Epoch: [4][4000/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9089 (2.8872)	Prec@1 54.000 (58.820)
Epoch: [4][4050/12812]	Time 0.209 (0.208)	Data 0.001 (0.001)	Loss 2.9261 (2.8872)	Prec@1 58.000 (58.819)
Epoch: [4][4100/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 3.0940 (2.8875)	Prec@1 52.000 (58.820)
Epoch: [4][4150/12812]	Time 0.206 (0.208)	Data 0.001 (0.001)	Loss 2.7121 (2.8876)	Prec@1 63.000 (58.810)
Epoch: [4][4200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9770 (2.8880)	Prec@1 58.000 (58.793)
Epoch: [4][4250/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7334 (2.8883)	Prec@1 64.000 (58.783)
Epoch: [4][4300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8171 (2.8886)	Prec@1 65.000 (58.777)
Epoch: [4][4350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0347 (2.8884)	Prec@1 57.000 (58.789)
Epoch: [4][4400/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8762 (2.8885)	Prec@1 60.000 (58.785)
Epoch: [4][4450/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8691 (2.8885)	Prec@1 65.000 (58.786)
Epoch: [4][4500/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.1799 (2.8884)	Prec@1 52.000 (58.791)
Epoch: [4][4550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.2349 (2.8886)	Prec@1 49.000 (58.792)
Epoch: [4][4600/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9867 (2.8887)	Prec@1 55.000 (58.794)
Epoch: [4][4650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8499 (2.8892)	Prec@1 57.000 (58.782)
Epoch: [4][4700/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8754 (2.8892)	Prec@1 60.000 (58.776)
Epoch: [4][4750/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8954 (2.8887)	Prec@1 54.000 (58.776)
Epoch: [4][4800/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.6805 (2.8885)	Prec@1 71.000 (58.786)
Epoch: [4][4850/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9097 (2.8886)	Prec@1 61.000 (58.787)
Epoch: [4][4900/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9583 (2.8887)	Prec@1 58.000 (58.789)
Epoch: [4][4950/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9990 (2.8887)	Prec@1 54.000 (58.788)
Epoch: [4][5000/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9191 (2.8886)	Prec@1 51.000 (58.787)
Epoch: [4][5050/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8982 (2.8889)	Prec@1 52.000 (58.784)
Epoch: [4][5100/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8804 (2.8894)	Prec@1 56.000 (58.766)
Epoch: [4][5150/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.0155 (2.8896)	Prec@1 54.000 (58.758)
Epoch: [4][5200/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8504 (2.8898)	Prec@1 60.000 (58.753)
Epoch: [4][5250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7251 (2.8899)	Prec@1 64.000 (58.755)
Epoch: [4][5300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0444 (2.8898)	Prec@1 56.000 (58.756)
Epoch: [4][5350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0037 (2.8898)	Prec@1 53.000 (58.751)
Epoch: [4][5400/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0954 (2.8903)	Prec@1 52.000 (58.748)
Epoch: [4][5450/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6324 (2.8902)	Prec@1 68.000 (58.753)
Epoch: [4][5500/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8530 (2.8901)	Prec@1 58.000 (58.759)
Epoch: [4][5550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9183 (2.8900)	Prec@1 56.000 (58.762)
Epoch: [4][5600/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0788 (2.8898)	Prec@1 49.000 (58.765)
Epoch: [4][5650/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7600 (2.8899)	Prec@1 63.000 (58.764)
Epoch: [4][5700/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9175 (2.8901)	Prec@1 59.000 (58.761)
Epoch: [4][5750/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8953 (2.8904)	Prec@1 66.000 (58.758)
Epoch: [4][5800/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.1664 (2.8906)	Prec@1 49.000 (58.753)
Epoch: [4][5850/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7514 (2.8907)	Prec@1 61.000 (58.761)
Epoch: [4][5900/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8825 (2.8905)	Prec@1 59.000 (58.769)
Epoch: [4][5950/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7030 (2.8906)	Prec@1 66.000 (58.772)
Epoch: [4][6000/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0572 (2.8908)	Prec@1 59.000 (58.771)
Epoch: [4][6050/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9674 (2.8907)	Prec@1 59.000 (58.776)
Epoch: [4][6100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7209 (2.8910)	Prec@1 66.000 (58.767)
Epoch: [4][6150/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.0361 (2.8909)	Prec@1 53.000 (58.768)
Epoch: [4][6200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9755 (2.8908)	Prec@1 57.000 (58.768)
Epoch: [4][6250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0570 (2.8908)	Prec@1 55.000 (58.769)
Epoch: [4][6300/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8202 (2.8909)	Prec@1 62.000 (58.771)
Epoch: [4][6350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9388 (2.8910)	Prec@1 58.000 (58.769)
Epoch: [4][6400/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8905 (2.8911)	Prec@1 59.000 (58.765)
Epoch: [4][6450/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7586 (2.8912)	Prec@1 58.000 (58.759)
Epoch: [4][6500/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9506 (2.8914)	Prec@1 61.000 (58.752)
Epoch: [4][6550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.2631 (2.8915)	Prec@1 53.000 (58.752)
Epoch: [4][6600/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8625 (2.8919)	Prec@1 66.000 (58.746)
Epoch: [4][6650/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.0573 (2.8924)	Prec@1 55.000 (58.730)
Epoch: [4][6700/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0249 (2.8925)	Prec@1 57.000 (58.726)
Epoch: [4][6750/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9764 (2.8925)	Prec@1 52.000 (58.720)
Epoch: [4][6800/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9587 (2.8926)	Prec@1 59.000 (58.715)
Epoch: [4][6850/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.1794 (2.8926)	Prec@1 48.000 (58.713)
Epoch: [4][6900/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7906 (2.8928)	Prec@1 60.000 (58.713)
Epoch: [4][6950/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0496 (2.8926)	Prec@1 52.000 (58.711)
Epoch: [4][7000/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0762 (2.8929)	Prec@1 53.000 (58.703)
Epoch: [4][7050/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.2414 (2.8932)	Prec@1 53.000 (58.697)
Epoch: [4][7100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9529 (2.8933)	Prec@1 52.000 (58.699)
Epoch: [4][7150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8217 (2.8933)	Prec@1 62.000 (58.703)
Epoch: [4][7200/12812]	Time 0.210 (0.207)	Data 0.001 (0.001)	Loss 2.6281 (2.8933)	Prec@1 66.000 (58.701)
Epoch: [4][7250/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7769 (2.8934)	Prec@1 61.000 (58.702)
Epoch: [4][7300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9137 (2.8933)	Prec@1 66.000 (58.706)
Epoch: [4][7350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0087 (2.8935)	Prec@1 60.000 (58.703)
Epoch: [4][7400/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0557 (2.8934)	Prec@1 58.000 (58.700)
Epoch: [4][7450/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.0504 (2.8934)	Prec@1 57.000 (58.696)
Epoch: [4][7500/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8745 (2.8933)	Prec@1 62.000 (58.703)
Epoch: [4][7550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.1869 (2.8936)	Prec@1 56.000 (58.695)
Epoch: [4][7600/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7831 (2.8935)	Prec@1 68.000 (58.700)
Epoch: [4][7650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7810 (2.8937)	Prec@1 63.000 (58.700)
Epoch: [4][7700/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0043 (2.8939)	Prec@1 53.000 (58.697)
Epoch: [4][7750/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.1373 (2.8940)	Prec@1 53.000 (58.692)
Epoch: [4][7800/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7485 (2.8941)	Prec@1 66.000 (58.689)
Epoch: [4][7850/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.1396 (2.8942)	Prec@1 54.000 (58.691)
Epoch: [4][7900/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.7328 (2.8945)	Prec@1 65.000 (58.691)
Epoch: [4][7950/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8397 (2.8944)	Prec@1 58.000 (58.693)
Epoch: [4][8000/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8179 (2.8945)	Prec@1 61.000 (58.690)
Epoch: [4][8050/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9570 (2.8945)	Prec@1 58.000 (58.689)
Epoch: [4][8100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8925 (2.8947)	Prec@1 58.000 (58.686)
Epoch: [4][8150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8645 (2.8948)	Prec@1 67.000 (58.680)
Epoch: [4][8200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7988 (2.8949)	Prec@1 63.000 (58.677)
Epoch: [4][8250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8966 (2.8950)	Prec@1 53.000 (58.675)
Epoch: [4][8300/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.0986 (2.8951)	Prec@1 52.000 (58.672)
Epoch: [4][8350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8512 (2.8952)	Prec@1 54.000 (58.667)
Epoch: [4][8400/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9595 (2.8952)	Prec@1 59.000 (58.668)
Epoch: [4][8450/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0983 (2.8953)	Prec@1 56.000 (58.664)
Epoch: [4][8500/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7486 (2.8954)	Prec@1 67.000 (58.664)
Epoch: [4][8550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0178 (2.8956)	Prec@1 58.000 (58.660)
Epoch: [4][8600/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 2.8791 (2.8956)	Prec@1 58.000 (58.661)
Epoch: [4][8650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9096 (2.8957)	Prec@1 56.000 (58.663)
Epoch: [4][8700/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 2.9134 (2.8958)	Prec@1 64.000 (58.656)
Epoch: [4][8750/12812]	Time 0.210 (0.207)	Data 0.001 (0.001)	Loss 2.9330 (2.8958)	Prec@1 52.000 (58.658)
Epoch: [4][8800/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8222 (2.8959)	Prec@1 61.000 (58.655)
Epoch: [4][8850/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8282 (2.8958)	Prec@1 62.000 (58.655)
Epoch: [4][8900/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8294 (2.8960)	Prec@1 62.000 (58.653)
Epoch: [4][8950/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6311 (2.8960)	Prec@1 66.000 (58.653)
Epoch: [4][9000/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7844 (2.8960)	Prec@1 67.000 (58.653)
Epoch: [4][9050/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8426 (2.8961)	Prec@1 67.000 (58.652)
Epoch: [4][9100/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8467 (2.8962)	Prec@1 56.000 (58.652)
Epoch: [4][9150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7750 (2.8961)	Prec@1 70.000 (58.649)
Epoch: [4][9200/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9139 (2.8964)	Prec@1 57.000 (58.645)
Epoch: [4][9250/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 2.6512 (2.8963)	Prec@1 57.000 (58.645)
Epoch: [4][9300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0889 (2.8965)	Prec@1 53.000 (58.638)
Epoch: [4][9350/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9935 (2.8965)	Prec@1 58.000 (58.640)
Epoch: [4][9400/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9516 (2.8966)	Prec@1 62.000 (58.639)
Epoch: [4][9450/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9995 (2.8969)	Prec@1 58.000 (58.637)
Epoch: [4][9500/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9792 (2.8968)	Prec@1 58.000 (58.641)
Epoch: [4][9550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9143 (2.8971)	Prec@1 55.000 (58.636)
Epoch: [4][9600/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9592 (2.8970)	Prec@1 56.000 (58.639)
Epoch: [4][9650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7654 (2.8972)	Prec@1 65.000 (58.635)
Epoch: [4][9700/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8644 (2.8972)	Prec@1 61.000 (58.637)
Epoch: [4][9750/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8875 (2.8972)	Prec@1 60.000 (58.637)
Epoch: [4][9800/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6920 (2.8973)	Prec@1 67.000 (58.632)
Epoch: [4][9850/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9340 (2.8973)	Prec@1 59.000 (58.632)
Epoch: [4][9900/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9902 (2.8975)	Prec@1 53.000 (58.629)
Epoch: [4][9950/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.1372 (2.8977)	Prec@1 56.000 (58.625)
Epoch: [4][10000/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6829 (2.8981)	Prec@1 61.000 (58.621)
Epoch: [4][10050/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0130 (2.8981)	Prec@1 53.000 (58.620)
Epoch: [4][10100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7503 (2.8983)	Prec@1 64.000 (58.622)
Epoch: [4][10150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0339 (2.8984)	Prec@1 52.000 (58.619)
Epoch: [4][10200/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 3.0234 (2.8986)	Prec@1 59.000 (58.614)
Epoch: [4][10250/12812]	Time 0.211 (0.207)	Data 0.001 (0.001)	Loss 2.9677 (2.8987)	Prec@1 64.000 (58.615)
Epoch: [4][10300/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.9372 (2.8988)	Prec@1 59.000 (58.612)
Epoch: [4][10350/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9714 (2.8988)	Prec@1 61.000 (58.612)
Epoch: [4][10400/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6253 (2.8989)	Prec@1 65.000 (58.613)
Epoch: [4][10450/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9443 (2.8990)	Prec@1 56.000 (58.606)
Epoch: [4][10500/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7712 (2.8991)	Prec@1 59.000 (58.604)
Epoch: [4][10550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9106 (2.8992)	Prec@1 56.000 (58.599)
Epoch: [4][10600/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9760 (2.8995)	Prec@1 52.000 (58.591)
Epoch: [4][10650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.1504 (2.8994)	Prec@1 53.000 (58.590)
Epoch: [4][10700/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 3.0166 (2.8995)	Prec@1 49.000 (58.586)
Epoch: [4][10750/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9845 (2.8998)	Prec@1 55.000 (58.580)
Epoch: [4][10800/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9073 (2.8997)	Prec@1 54.000 (58.585)
Epoch: [4][10850/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8487 (2.8999)	Prec@1 62.000 (58.583)
Epoch: [4][10900/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.6820 (2.8998)	Prec@1 64.000 (58.583)
Epoch: [4][10950/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0709 (2.8999)	Prec@1 59.000 (58.583)
Epoch: [4][11000/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8556 (2.9000)	Prec@1 57.000 (58.578)
Epoch: [4][11050/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 2.9801 (2.9001)	Prec@1 52.000 (58.578)
Epoch: [4][11100/12812]	Time 0.209 (0.207)	Data 0.001 (0.001)	Loss 2.6872 (2.9000)	Prec@1 68.000 (58.580)
Epoch: [4][11150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8349 (2.9001)	Prec@1 60.000 (58.579)
Epoch: [4][11200/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7385 (2.9002)	Prec@1 64.000 (58.577)
Epoch: [4][11250/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0030 (2.9005)	Prec@1 59.000 (58.574)
Epoch: [4][11300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7306 (2.9006)	Prec@1 66.000 (58.573)
Epoch: [4][11350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9480 (2.9007)	Prec@1 54.000 (58.575)
Epoch: [4][11400/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7002 (2.9008)	Prec@1 64.000 (58.570)
Epoch: [4][11450/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8252 (2.9008)	Prec@1 60.000 (58.572)
Epoch: [4][11500/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8365 (2.9010)	Prec@1 61.000 (58.568)
Epoch: [4][11550/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8367 (2.9010)	Prec@1 63.000 (58.570)
Epoch: [4][11600/12812]	Time 0.227 (0.207)	Data 0.024 (0.001)	Loss 2.8831 (2.9011)	Prec@1 53.000 (58.566)
Epoch: [4][11650/12812]	Time 0.205 (0.207)	Data 0.001 (0.001)	Loss 2.7941 (2.9011)	Prec@1 62.000 (58.568)
Epoch: [4][11700/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.8901 (2.9013)	Prec@1 63.000 (58.564)
Epoch: [4][11750/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9555 (2.9014)	Prec@1 57.000 (58.564)
Epoch: [4][11800/12812]	Time 0.213 (0.207)	Data 0.001 (0.001)	Loss 2.8975 (2.9014)	Prec@1 56.000 (58.566)
Epoch: [4][11850/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.7161 (2.9016)	Prec@1 60.000 (58.564)
Epoch: [4][11900/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8732 (2.9016)	Prec@1 64.000 (58.567)
Epoch: [4][11950/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0103 (2.9015)	Prec@1 54.000 (58.570)
Epoch: [4][12000/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9817 (2.9016)	Prec@1 65.000 (58.569)
Epoch: [4][12050/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.4040 (2.9018)	Prec@1 46.000 (58.568)
Epoch: [4][12100/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8873 (2.9018)	Prec@1 55.000 (58.568)
Epoch: [4][12150/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0381 (2.9020)	Prec@1 55.000 (58.563)
Epoch: [4][12200/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.8454 (2.9021)	Prec@1 55.000 (58.563)
Epoch: [4][12250/12812]	Time 0.206 (0.207)	Data 0.001 (0.001)	Loss 2.9358 (2.9023)	Prec@1 64.000 (58.559)
Epoch: [4][12300/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8164 (2.9023)	Prec@1 57.000 (58.557)
Epoch: [4][12350/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.9971 (2.9025)	Prec@1 57.000 (58.555)
Epoch: [4][12400/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0229 (2.9026)	Prec@1 59.000 (58.554)
Epoch: [4][12450/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0056 (2.9027)	Prec@1 56.000 (58.557)
Epoch: [4][12500/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.3595 (2.9028)	Prec@1 45.000 (58.554)
Epoch: [4][12550/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8381 (2.9029)	Prec@1 58.000 (58.549)
Epoch: [4][12600/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 3.0140 (2.9030)	Prec@1 58.000 (58.548)
Epoch: [4][12650/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.8020 (2.9032)	Prec@1 61.000 (58.542)
Epoch: [4][12700/12812]	Time 0.208 (0.207)	Data 0.001 (0.001)	Loss 2.6984 (2.9033)	Prec@1 69.000 (58.542)
Epoch: [4][12750/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.7977 (2.9034)	Prec@1 63.000 (58.539)
Epoch: [4][12800/12812]	Time 0.207 (0.207)	Data 0.001 (0.001)	Loss 2.6915 (2.9035)	Prec@1 66.000 (58.537)
Test: [0/500]	Time 0.056 (0.056)	Loss 1.0190 (1.0190)	Prec@1 88.000 (88.000)
Test: [50/500]	Time 0.057 (0.057)	Loss 1.1410 (1.4026)	Prec@1 80.000 (75.137)
Test: [100/500]	Time 0.056 (0.057)	Loss 1.9074 (1.4848)	Prec@1 47.000 (72.109)
Test: [150/500]	Time 0.057 (0.057)	Loss 0.9004 (1.4778)	Prec@1 87.000 (71.947)
Test: [200/500]	Time 0.056 (0.057)	Loss 2.1044 (1.4801)	Prec@1 58.000 (72.124)
Test: [250/500]	Time 0.056 (0.057)	Loss 2.3533 (1.6184)	Prec@1 53.000 (69.295)
Test: [300/500]	Time 0.056 (0.057)	Loss 3.4078 (1.6927)	Prec@1 39.000 (67.834)
Test: [350/500]	Time 0.056 (0.057)	Loss 1.7843 (1.7567)	Prec@1 65.000 (66.393)
Test: [400/500]	Time 0.057 (0.057)	Loss 1.1917 (1.8131)	Prec@1 80.000 (65.357)
Test: [450/500]	Time 0.056 (0.057)	Loss 1.6665 (1.8616)	Prec@1 71.000 (64.308)
 * Prec@1 64.644
